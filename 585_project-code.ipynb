{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "585_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIyUd4AR89og",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounts data with stem as 'drive'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rHYb0aAB7C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packages to install\n",
        "!pip install PyPDF2\n",
        "!pip install textract\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPaMpwhR8p30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "# General\n",
        "import copy\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import *\n",
        "\n",
        "# Tokenization\n",
        "import PyPDF2 \n",
        "import textract\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Classification\n",
        "import sklearn\n",
        "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier as rfc\n",
        "from sklearn.neural_network import MLPClassifier as mlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byL-iQrF9XPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This line refreshes the known data in the resume folder\n",
        "!ls \"/content/drive/Shared drives/585_final_project/HackHer413_Resumes/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omisQ0QH9VU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up path for resume scraper\n",
        "mypath = \"/content/drive/Shared drives/585_final_project/HackHer413_Resumes/\"\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2f8WsfByZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = []\n",
        "for filename in onlyfiles:\n",
        "  f = filename\n",
        "  filename = mypath+filename\n",
        "  print(filename)\n",
        "  #open allows you to read the file\n",
        "  pdfFileObj = open(filename,'rb')\n",
        "  #The pdfReader variable is a readable object that will be parsed\n",
        "  pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict=False)\n",
        "  #discerning the number of pages will allow us to parse through all #the pages\n",
        "  num_pages = pdfReader.numPages\n",
        "  count = 0\n",
        "  text = \"\"\n",
        "  #The while loop will read each page\n",
        "  while count < num_pages:\n",
        "      pageObj = pdfReader.getPage(count)\n",
        "      count +=1\n",
        "      text += pageObj.extractText()\n",
        "      text = text.replace('\\r','!')\n",
        "      text = text.replace('\\n','')\n",
        "      text = text.replace('\\t','^')\n",
        "      text = text.replace('\\v','*')\n",
        "      text = text.lower()\n",
        "  # split into words by white space\n",
        "  # split into words by white space\n",
        "\n",
        "  # remove punctuation from each word\n",
        "  import re\n",
        "  words = re.split(r'\\W+', text)\n",
        "  #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
        "  if text != \"\":\n",
        "     text = text\n",
        "  #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
        "  else:\n",
        "    try:\n",
        "      text = textract.process(fileurl, method='tesseract', language='eng')\n",
        "    except:\n",
        "      text = \"\"\n",
        "  # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
        "  # Now, we will clean our text variable, and return it as a list of keywords.\n",
        "\n",
        "  texts.append((f, text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXvWDArmQ2ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Size of raw dataset:', len(texts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uTkeZcLcLMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "anonymize:\n",
        "\n",
        "DESCRIPTION:\n",
        "takes in tokenized resume and removes identifying information. Approaches task \n",
        "by removing all text before a few 'action' words. This process\n",
        "also conveniently cleans the data of a few garbage tokens.\n",
        "\n",
        "PARAMS:\n",
        "keywords - tokenized data from a single resume\n",
        "\n",
        "RETURN:\n",
        "a copy of keywords with id info scrubbed\n",
        "\n",
        "'''\n",
        "def anonymize(keywords):\n",
        "  lk = len(keywords)\n",
        "  keewords = copy.copy(keywords)\n",
        "  education = ['Education', 'education', 'EDUCATION']\n",
        "  school = ['School', 'school', 'SCHOOL']\n",
        "  experience = ['Experience', 'experience', 'EXPERIENCE']\n",
        "  skills = ['Skills', 'skills', 'SKILLS']\n",
        "  technical = ['Technical', 'technical', 'TECHNICAL']\n",
        "  research = ['Research', 'research', 'RESEARCH']\n",
        "  projects = ['Projects', 'projects', 'PROJECTS']\n",
        "  objective = ['Objective', 'objective', 'OBJECTIVE']\n",
        "  activities = ['Activities', 'activities', 'ACTIVITIES']\n",
        "  interests = ['Interests', 'interests', 'INTERESTS']\n",
        "  for word in range(lk):\n",
        "    if (keywords[word] in education or keywords[word] in experience or\n",
        "        keywords[word] in skills or keywords[word] in technical or\n",
        "       keywords[word] in research or keywords[word] in projects or\n",
        "       keywords[word] in objective or keywords[word] in activities or\n",
        "       keywords[word] in interests):\n",
        "      break\n",
        "    else:\n",
        "      keewords = keewords[1:]\n",
        "  return keewords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm1h7C4R5Poz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function for categorize\n",
        "def make_false(flag_array, target):\n",
        "  flag_arr = copy.copy(flag_array)\n",
        "  for flag in flag_arr:\n",
        "    if flag is not target:\n",
        "      flag[0] = False\n",
        "  return flag_arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUdiY7JIkGt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "categorize:\n",
        "\n",
        "DESCRIPTION:\n",
        "Sorts anonymized data into general resume categories retaining order\n",
        "\n",
        "PARAMS:\n",
        "keywords - anonymized list of resume data in order-ish\n",
        "\n",
        "RETURN:\n",
        "a dictionary of the categorized resume\n",
        "'''\n",
        "def categorize(keywords):\n",
        "  education = ['Education', 'education', 'EDUCATION', 'School', 'school', 'SCHOOL']\n",
        "  \n",
        "  # Flag to determine both if we run into the word and are in the \n",
        "  # section (flag[0]), as well as if we have seen it before (flag[1])\n",
        "  # Given nature of reumes, first time we encounter these words is \n",
        "  # overwhelmingly the section header\n",
        "  edu = [False, False]\n",
        "  experience = ['Experience', 'experience', 'EXPERIENCE']\n",
        "  exp = [False, False]\n",
        "  skills = ['Skills', 'skills', 'SKILLS', 'Technical', 'technical', 'TECHNICAL']\n",
        "  tech = [False, False]\n",
        "  research = ['Research', 'research', 'RESEARCH']\n",
        "  res = [False, False]\n",
        "  projects = ['Projects', 'projects', 'PROJECTS']\n",
        "  pro = [False, False]\n",
        "  objective = ['Objective', 'objective', 'OBJECTIVE']\n",
        "  obj = [False, False]\n",
        "  activities = ['Activities', 'activities', 'ACTIVITIES']\n",
        "  act = [False, False]\n",
        "  interests = ['Interests', 'interests', 'INTERESTS']\n",
        "  inter = [False, False]\n",
        "  flags = [edu, exp, tech, res, pro, obj, act, inter]\n",
        "  categories_without_skills_and_tech = ['Education', 'education', 'EDUCATION',\n",
        "                                        'School', 'school', 'SCHOOL'\n",
        "                                       'Experience', 'experience', 'EXPERIENCE',\n",
        "                                       'Research', 'research', 'RESEARCH',\n",
        "                                        'Projects', 'projects', 'PROJECTS',\n",
        "                                        'Objective', 'objective', 'OBJECTIVE',\n",
        "                                        'Activities', 'activities', 'ACTIVITIES',\n",
        "                                        'Interests', 'interests', 'INTERESTS']\n",
        "  all_cats = ['Education', 'education', 'EDUCATION',\n",
        "              'School', 'school', 'SCHOOL'\n",
        "              'Experience', 'experience', 'EXPERIENCE',\n",
        "              'Skills', 'skills', 'SKILLS',\n",
        "              'Technical', 'technical', 'TECHNICAL',\n",
        "              'Research', 'research', 'RESEARCH',\n",
        "              'Projects', 'projects', 'PROJECTS',\n",
        "              'Objective', 'objective', 'OBJECTIVE',\n",
        "              'Activities', 'activities', 'ACTIVITIES',\n",
        "              'Interests', 'interests', 'INTERESTS']\n",
        "  \n",
        "  categories = {'education':[], 'experience':[], 'skills':[], 'research':[],\n",
        "                'projects':[], 'objective':[], 'activities':[], 'interests':[]}\n",
        "  words = copy.copy(keywords)\n",
        "  '''\n",
        "  this counter + counter_val are to prevent accidentally going into the next\n",
        "  section\n",
        "  ** in future be sure to check for 'research intern' or 'research assistant' **\n",
        "  '''\n",
        "  counter = 0\n",
        "  count_val = 3\n",
        "  for word in words:\n",
        "    if (word in education and edu[1] == False and counter <= 0):\n",
        "      edu[0] = True\n",
        "      edu[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, edu)\n",
        "    elif (word in experience and exp[1] == False and counter <= 0):\n",
        "      exp[0] = True\n",
        "      exp[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, exp)\n",
        "    elif (word in skills and tech[1] == False and counter <= 0):\n",
        "      tech[0] = True\n",
        "      tech[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, tech)\n",
        "    elif (word in research and res[1] == False and counter <= 0):\n",
        "      res[0] = True\n",
        "      res[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, res)\n",
        "    elif (word in projects and pro[1] == False and counter <= 0):\n",
        "      pro[0] = True\n",
        "      pro[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, pro)\n",
        "    elif (word in objective and obj[1] == False and counter <= 0):\n",
        "      obj[0] = True\n",
        "      obj[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, obj)\n",
        "    elif (word in activities and act[1] == False and counter <= 0):\n",
        "      act[0] = True\n",
        "      act[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, act)\n",
        "    elif (word in interests and inter[1] == False and counter <= 0):\n",
        "      inter[0] = True\n",
        "      inter[1] = True\n",
        "      counter = count_val\n",
        "      flags = make_false(flags, inter)\n",
        "    \n",
        "    if (edu[0] and word not in education):\n",
        "      categories['education'].append(word)\n",
        "      counter -=1\n",
        "    if (exp[0] and word not in experience):\n",
        "      categories['experience'].append(word)\n",
        "      counter -=1\n",
        "    if (tech[0] and word not in skills):\n",
        "      categories['skills'].append(word)\n",
        "      counter -=1\n",
        "    if (res[0] and word not in research):\n",
        "      categories['research'].append(word)\n",
        "      counter -=1\n",
        "    if (pro[0] and word not in projects):\n",
        "      categories['projects'].append(word)\n",
        "      counter -=1\n",
        "    if (obj[0] and word not in objective):\n",
        "      categories['objective'].append(word)\n",
        "      counter -=1\n",
        "    if (act[0] and word not in activities):\n",
        "      categories['activities'].append(word)\n",
        "      counter -=1\n",
        "    if (inter[0] and word not in interests):\n",
        "      categories['interests'].append(word)\n",
        "      counter -=1\n",
        "  return categories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8saIuajD3LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_keywords = []\n",
        "tokenized_categories = []\n",
        "success_files = []\n",
        "c = 0\n",
        "for t in texts:\n",
        "  #The word_tokenize() function will break our text phrases into #individual words\n",
        "  tokens = word_tokenize(t[1])\n",
        "  #we'll create a new list which contains punctuation we wish to clean\n",
        "  punctuations = ['(',')',';',':','[',']',',']\n",
        "  #We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
        "  stop_words = stopwords.words('english')\n",
        "  #We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
        "  keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n",
        "  if keywords != []:\n",
        "    k = anonymize(keywords)\n",
        "    cats = categorize(k)\n",
        "    if k == []:\n",
        "      c += 1\n",
        "    else:\n",
        "      tokenized_keywords.append((t[0], k))\n",
        "      tokenized_categories.append((t[0],cats))\n",
        "      success_files.append(t[0])\n",
        "\n",
        "print('Size of cleaned dataset:',len(tokenized_keywords))\n",
        "print(tokenized_categories[0:2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sLPiPgwRhOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Small test to see how we're doing\n",
        "# Should not see any names or identifying informations (other than the file handle which could have names)\n",
        "tokenized_categories[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toEF909U7qTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads 50-dimensional glove embeddings file from wikipedia\n",
        "def loadGloveModel(gloveFile):\n",
        "    print(\"Loading Glove Model\")\n",
        "    f = open(gloveFile,'r')\n",
        "    model = {}\n",
        "    for line in f:\n",
        "        splitLine = line.split()\n",
        "        word = splitLine[0]\n",
        "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
        "        model[word] = embedding\n",
        "    print(\"Done.\",len(model),\" words loaded!\")\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnWt1U2F-5gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute above function\n",
        "glove_file = \"/content/drive/Shared drives/585_final_project/glove_words.txt\"\n",
        "glove_model = loadGloveModel(glove_file)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaBBaupmNr3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity check to make sure model was loaded in correctly\n",
        "print('the' in glove_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2emHFQULXj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are assigning a glove word embedding to its corresponding word in the \n",
        "# resume for all resumes\n",
        "model_embeddings = {}\n",
        "for tokenized_resume in tokenized_categories:\n",
        "  resume_embeddings = {}\n",
        "  for cat in tokenized_resume[1]:\n",
        "    category_embeddings = {}\n",
        "    for word in tokenized_resume[1][cat]:\n",
        "      if word in glove_model:\n",
        "        if word in category_embeddings:\n",
        "          index = category_embeddings[word][0]\n",
        "          index += 1\n",
        "          # Change glove_model[word] to generate_random_embedding()\n",
        "          # to create the random embedding baseline\n",
        "          category_embeddings[word] = (index, glove_model[word])\n",
        "        else:\n",
        "          # Change glove_model[word] to generate_random_embedding()\n",
        "          # to create the random embedding baseline\n",
        "          category_embeddings[word] = (1, glove_model[word])\n",
        "    if cat in resume_embeddings:\n",
        "      index = resume_embeddings[cat][0]\n",
        "      index += 1\n",
        "      resume_embeddings[cat] = (index, category_embeddings)\n",
        "    else:\n",
        "      resume_embeddings[cat] = (1, category_embeddings)\n",
        "  if tokenized_resume[0] in model_embeddings:\n",
        "    index = model_embeddings[tokenized_resume[0]][0]\n",
        "    index += 1\n",
        "    model_embeddings[tokenized_resume[0]] = (index, resume_embeddings)\n",
        "  else:\n",
        "    model_embeddings[tokenized_resume[0]] = (1, resume_embeddings)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jhIvdMzL7u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates a completely random 50-dimensional word embedding for every instance\n",
        "# of a word in the corpus. Two of the same words could have a different\n",
        "# embedding with this implementation\n",
        "def generate_random_embedding():\n",
        "    embedding = []\n",
        "    for i in range(50):\n",
        "        random_number = random.uniform(-1,1)\n",
        "        embedding.append(random_number)\n",
        "    return np.array(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db-XI1Z5YSBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity check to make sure it creates the embeddings as expected\n",
        "model_embeddings['100_Tanisha_Nalavadi.pdf']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGU3vsL-XwzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If a word appears more than once in a resume, we want it to be treated\n",
        "# differently than if it appears only once. This code block allows us to do that\n",
        "single_embeddings = copy.deepcopy(model_embeddings)\n",
        "for resume in single_embeddings:\n",
        "  for activity in single_embeddings[resume][1]:\n",
        "    for word_embedding in single_embeddings[resume][1][activity][1]:\n",
        "      single_embeddings[resume][1][activity][1][word_embedding] = single_embeddings[resume][1][activity][1][word_embedding][0] * single_embeddings[resume][1][activity][1][word_embedding][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YuK6spMdJWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity check to make sure it creates the embeddings as expected\n",
        "single_embeddings['100_Tanisha_Nalavadi.pdf']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSzRpNCjessP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are taking the mean of the word embeddings for each category to form\n",
        "# single category embeddings. From there we concatenate the category embeddings \n",
        "# together to form a resume embedding\n",
        "resume_embeddings = []\n",
        "for resume in single_embeddings:\n",
        "  res_embedding = [resume, np.array([])]\n",
        "  for activity in single_embeddings[resume][1]:\n",
        "    activity_sum = np.zeros(50)\n",
        "    counter = 0\n",
        "    for word in single_embeddings[resume][1][activity][1]:\n",
        "      activity_sum = np.add( activity_sum, single_embeddings[resume][1][activity][1][word])\n",
        "      counter += 1\n",
        "    if (counter != 0):\n",
        "      activity_sum = activity_sum/counter\n",
        "    res_embedding[1] = np.concatenate((res_embedding[1], activity_sum))\n",
        "  resume_embeddings.append([res_embedding[0], res_embedding[1].tolist()])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmfRe5RNO6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity test\n",
        "for i in range(5):\n",
        "  print(resume_embeddings[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgC2239kSl0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# yx_label contains list of tuples\n",
        "# add new directory name in directory_names (if add new directory in the future)\n",
        "yx_label = []\n",
        "directory_names = [('AI', 0),('fullstack_SWE', 1), ('Hardware', 2), ('Informatics', 3), ('Other', 4), ('inexperienced_SWE', 5),('Web_Developer', 6)]\n",
        "for dir_name in directory_names:\n",
        "  temp = \"/content/drive/Shared drives/585_final_project/\"+dir_name[0]+\"/\"\n",
        "  filename_list = [f for f in listdir(temp) if isfile(join(temp, f))]\n",
        "  for name in filename_list:\n",
        "    yx_label.append((dir_name[1],name))\n",
        "print(yx_label[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIyOraznVc4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# finds the class label from the folders and assigns it to the file name\n",
        "# returns in a list of tuples with the stored data\n",
        "def find(file_name, list_of_files):\n",
        "  for f in list_of_files:\n",
        "    if (f[1] == file_name):\n",
        "      return f\n",
        "  return (4, file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nPXN3I_TbAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attatches the class label to the embedding data\n",
        "labeled_resume_embeddings = copy.deepcopy(resume_embeddings)\n",
        "for r in labeled_resume_embeddings:\n",
        "  f = find(r[0], yx_label)[0]\n",
        "  r.insert(0, f)\n",
        "print(labeled_resume_embeddings[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKM68zTEYlD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format training and testing data\n",
        "y_data = []\n",
        "x_data = []\n",
        "combined_data = []\n",
        "for resume in labeled_resume_embeddings:\n",
        "  combined_data.append((resume[0], resume[2]))\n",
        "random.shuffle(combined_data)\n",
        "for resume in combined_data:\n",
        "  y_data.append(resume[0])\n",
        "  x_data.append(resume[1])\n",
        "x_train = np.array(x_data[:int((len(x_data)/5)*4)])\n",
        "x_test = np.array(x_data[int((len(x_data)/5)*4):])\n",
        "y_train = np.array(y_data[:int((len(y_data)/5)*4)])\n",
        "y_test = np.array(y_data[int((len(y_data)/5)*4):])\n",
        "\n",
        "# Print shapes of sets as a sanity check\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnMwqTgMa-Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifiers\n",
        "gradient_boost_classifier = gbc()\n",
        "random_forest_classifier = rfc()\n",
        "multilayer_perceptron_classifier = mlp()\n",
        "grid_params = {\"n_estimators\":[5, 7, 10, 15]}\n",
        "search = GridSearchCV(gradient_boost_classifier, grid_params , scoring=\"accuracy\",\n",
        "                    return_train_score=True, refit=True, cv=5)\n",
        "search.fit(x_train, y_train)\n",
        "results = search.cv_results_\n",
        "print(results.get('params'))\n",
        "print(results.get('mean_test_score'))\n",
        "print(results.get('mean_train_score'))\n",
        "print(search.best_params_)\n",
        "print(search.best_score_)\n",
        "\n",
        "grid_params_2 = {\"n_estimators\":[5, 7, 10, 15]}\n",
        "search_2 = GridSearchCV(random_forest_classifier, grid_params , scoring=\"accuracy\",\n",
        "                    return_train_score=True, refit=True, cv=5)\n",
        "search_2.fit(x_train, y_train)\n",
        "results_2 = search_2.cv_results_\n",
        "print(results_2.get('params'))\n",
        "print(results_2.get('mean_test_score'))\n",
        "print(results_2.get('mean_train_score'))\n",
        "print(search_2.best_params_)\n",
        "print(search_2.best_score_)\n",
        "\n",
        "grid_params = {\"activation\":['tanh', 'relu'], \"alpha\":[5, 6, 7, 8, 9, 10], \"learning_rate\":[\"constant\", \"adaptive\"]}\n",
        "mlp_classifier = GridSearchCV(multilayer_perceptron_classifier, grid_params , scoring=\"accuracy\",\n",
        "                    return_train_score=True, refit=True, cv=3)\n",
        "mlp_classifier.fit(x_train, y_train)\n",
        "results = mlp_classifier.cv_results_\n",
        "print(results.get('params'))\n",
        "print(results.get('mean_test_score'))\n",
        "print(results.get('mean_train_score'))\n",
        "print(mlp_classifier.best_params_)\n",
        "print(mlp_classifier.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shr-93MaKMsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See the accuracy on the test set\n",
        "print(\"Gradient Boost accuracy: \",search.score(x_test, y_test))\n",
        "print(\"Random Forest accuracy: \",search_2.score(x_test, y_test))\n",
        "print(\"Multilayer Perceptron Model accuracy: \",mlp_classifier.score(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htHpH1Y2R2gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ELMo time!\n",
        "!pip install allennlp\n",
        "import allennlp\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "\n",
        "Elmo = ElmoEmbedder()\n",
        "tokens = [\"Roasted\", \"ants\", \"are\", \"a\", \"popular\", \"snack\", \"in\", \"Columbia\"]\n",
        "vectors = Elmo.embed_sentence(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpqVExN5UXJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity check\n",
        "vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A67vM3DVs-ZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Similar labeling process from above with GloVe\n",
        "content = copy.deepcopy(tokenized_categories)\n",
        "for resume in range(len(content)):\n",
        "  category = find(content[resume][0], yx_label)[0]\n",
        "  content[resume] = (category, content[resume][0], content[resume][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV-862PLJfJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity check\n",
        "print(content[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po_-zAlX3tKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train ELMo Model\n",
        "elmo_embeddings = []\n",
        "labeled_content = copy.deepcopy(content)\n",
        "count = 0\n",
        "for resume in labeled_content:\n",
        "  elmo_embedding = []\n",
        "  for category in resume[2]:\n",
        "    if (resume[2][category] != []):\n",
        "      embeddings = Elmo.embed_sentence(resume[2][category])\n",
        "      embeddings = np.mean(embeddings, axis=0)\n",
        "      obj_to_add = (resume[1], embeddings)\n",
        "      m = np.mean(obj_to_add[1], axis = 0)\n",
        "      elmo_embedding.append(m)\n",
        "    else:\n",
        "      elmo_embedding.append(np.zeros((1, 1024)))\n",
        "  elmo_embeddings.append((resume[1],elmo_embedding))\n",
        "  print(str(count)+\"/\"+str(len(labeled_content)))\n",
        "  count += 1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ifi2l9td40v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Squish down 3-d ELMo output matrix using numpy mean\n",
        "c_elmo = copy.deepcopy(elmo_embeddings)\n",
        "e_embeddings = []\n",
        "for resume in c_elmo:\n",
        "  concat_array = np.array([])\n",
        "  # print(resume)\n",
        "  for category in resume[1]:\n",
        "    # print(category.shape)\n",
        "    sum_array = np.zeros((1, 1024))\n",
        "    for word in category:\n",
        "      sum_array = np.add(sum_array, word)\n",
        "    sum_array = sum_array/(len(category))\n",
        "    # print(sum_array[0].shape)\n",
        "    # print(concat_array)\n",
        "    concat_array = np.concatenate((concat_array, sum_array[0]))\n",
        "  e_embeddings.append((resume[0], concat_array))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KUpjJK5jSVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sanity check\n",
        "print(e_embeddings[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iWAWfelbMIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format Data for  classifiers\n",
        "c_elmoo = copy.deepcopy(e_embeddings)\n",
        "y_trains = []\n",
        "for resume in c_elmoo:\n",
        "  y_val = [find(resume[0], yx_label)[0], resume[1]]\n",
        "  y_trains.append(y_val)\n",
        "y_data = []\n",
        "x_data = []\n",
        "combined_data = []\n",
        "for yx in y_trains:\n",
        "  combined_data.append((yx[0], yx[1]))\n",
        "random.shuffle(combined_data)\n",
        "for yx in combined_data:\n",
        "  y_data.append(yx[0])\n",
        "  x_data.append(yx[1])\n",
        "x_train = x_data[:int((len(x_data)/5)*4)]\n",
        "x_test = x_data[int((len(x_data)/5)*4):]\n",
        "y_train = y_data[:int((len(y_data)/5)*4)]\n",
        "y_test = y_data[int((len(y_data)/5)*4):]\n",
        "print(len(x_train))\n",
        "print(len(x_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))\n",
        "print(y_test)\n",
        "# print(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K11u6duadoTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifiers\n",
        "gradient_boost_classifier = gbc()\n",
        "random_forest_classifier = rfc()\n",
        "\n",
        "grid_params = {\"n_estimators\":[10, 20, 30, 35]}\n",
        "search = GridSearchCV(gradient_boost_classifier, grid_params , scoring=\"accuracy\",\n",
        "                    return_train_score=True, refit=True, cv=5)\n",
        "search.fit(x_train, y_train)\n",
        "# search.fit(x_data, y_data)\n",
        "results = search.cv_results_\n",
        "print(results.get('params'))\n",
        "print(results.get('mean_test_score'))\n",
        "print(results.get('mean_train_score'))\n",
        "print(search.best_params_)\n",
        "print(search.best_score_)\n",
        "\n",
        "grid_params_2 = {\"n_estimators\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
        "search_2 = GridSearchCV(random_forest_classifier, grid_params , scoring=\"accuracy\",\n",
        "                    return_train_score=True, refit=True, cv=5)\n",
        "search_2.fit(x_train, y_train)\n",
        "results_2 = search_2.cv_results_\n",
        "print(results_2.get('params'))\n",
        "print(results_2.get('mean_test_score'))\n",
        "print(results_2.get('mean_train_score'))\n",
        "print(search_2.best_params_)\n",
        "print(search_2.best_score_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkpJQIziAAtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Gradient Boost accuracy: \",search.score(x_test, y_test))\n",
        "print(\"Random Forest accuracy: \",search_2.score(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGXaqscnHRD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "multilayer_perceptron_classifier = mlp()\n",
        "grid_params = {\"activation\":['tanh'], \"alpha\":[0.001, 0.01, 0.1, 1, 3, 5, 6, 7, 8], \"learning_rate\":[\"constant\"]}\n",
        "mlp_classifier = GridSearchCV(multilayer_perceptron_classifier, grid_params , scoring=\"accuracy\",\n",
        "                    return_train_score=True, refit=True, cv=5)\n",
        "mlp_classifier.fit(x_train, y_train)\n",
        "results = mlp_classifier.cv_results_\n",
        "print(results.get('params'))\n",
        "print(results.get('mean_test_score'))\n",
        "print(results.get('mean_train_score'))\n",
        "print(mlp_classifier.best_params_)\n",
        "print(mlp_classifier.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuqKrG7cHcdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Multilayer Perceptron Model accuracy: \",mlp_classifier.score(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}