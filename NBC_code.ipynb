{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hlerner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hlerner/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import copy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.naive_bayes import GaussianNB as nbc\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import *\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100_Tanisha_Nalavadi.pdf\r\n",
      "1016_Akul_Siddalingaswamy.pdf\r\n",
      "101_Devika_Chipalkatti.pdf\r\n",
      "102_Narpavirajan_Venkatesan_Muralidharan.pdf\r\n",
      "104_Manisha_Barnwal.pdf\r\n",
      "105_Shirley_Xu.pdf\r\n",
      "106_Khanh_Ngo.pdf\r\n",
      "107_SREEPARNA_MUKHERJEE.pdf\r\n",
      "108_Saumya_Priya.pdf\r\n",
      "109_Paulina_Valdivieso.pdf\r\n",
      "111_Ester_Zhao.pdf\r\n",
      "112_Karen_Santamaria.pdf\r\n",
      "113_YuanYi_Chen.pdf\r\n",
      "114_Joanna_Olson.pdf\r\n",
      "115_Chesta_Singh.pdf\r\n",
      "116_Laurel_Parsons.pdf\r\n",
      "118_Ilika_Tripathi.pdf\r\n",
      "119_Lemara_Williams.pdf\r\n",
      "120_Nathacha_Almanzar.pdf\r\n",
      "121_Grace_Seward.pdf\r\n",
      "122_Jaime_Woolcock.pdf\r\n",
      "124_Vedantika_Chaudhary.pdf\r\n",
      "126_Ophelia_Mendoza.pdf\r\n",
      "128_Sarah_Andraka.pdf\r\n",
      "130_Mehak_Lodhi.pdf\r\n",
      "131_Leah_Leshchinsky.pdf\r\n",
      "132_Gabrielle_Newman.pdf\r\n",
      "134_Marissa_D-Alonzo.pdf\r\n",
      "136_Mary_Kate_Murtagh.pdf\r\n",
      "138_Grainne_Casey.pdf\r\n",
      "139_Rupam_Tiwari.pdf\r\n",
      "140_Juliana_Lugg.pdf\r\n",
      "142_snehitha_Ramasahayam.pdf\r\n",
      "143_amy_guo.pdf\r\n",
      "144_Laura_O_Malley.pdf\r\n",
      "145_Sydney_Smith.pdf\r\n",
      "146_Emily_Earl.pdf\r\n",
      "148_Yomna_Barakat.pdf\r\n",
      "149_Danielle_Finestone.pdf\r\n",
      "152_Akshaya_Rajagopalan.pdf\r\n",
      "153_Megan_Cruz.pdf\r\n",
      "155_Sarah_Chou.pdf\r\n",
      "156_Lena_Cohen.pdf\r\n",
      "158_Hea_Rim_Yoon.pdf\r\n",
      "159_Marlena_Keisler.pdf\r\n",
      "160_Elizabeth_Wang.pdf\r\n",
      "161_Dhruvi_Vora.pdf\r\n",
      "162_Sreeya_Sai.pdf\r\n",
      "163_Hannah_Rogers.pdf\r\n",
      "164_Riya_Dhar.pdf\r\n",
      "166_Sabrina_Taylor.pdf\r\n",
      "168_Tremelle_Lester.pdf\r\n",
      "171_Michela_Marchini.pdf\r\n",
      "174_Caitlin_Pichette.pdf\r\n",
      "175_Colleen_Dunlap.pdf\r\n",
      "176_Afreen_Aliya.pdf\r\n",
      "177_Claire_Jordan.pdf\r\n",
      "178_Yooni_Park.pdf\r\n",
      "180_Catherine_Yeo.pdf\r\n",
      "182_Sharon_Jiang.pdf\r\n",
      "183_Bipasha_Chowdhury.pdf\r\n",
      "184_Dyuti_De.pdf\r\n",
      "186_Corinne_Greene.pdf\r\n",
      "188_Diane_Tam.pdf\r\n",
      "189_Gabriella_Lalli_Martins.pdf\r\n",
      "190_Amanda_Murray.pdf\r\n",
      "191_Shreya_Parashare.pdf\r\n",
      "192_Sophie_Vincent.pdf\r\n",
      "194_Lauren_McKim.pdf\r\n",
      "195_Hannah_Fraser.pdf\r\n",
      "197_Frankie_Rourke.pdf\r\n",
      "19_Kashish_Arora.pdf\r\n",
      "200_Radhika_Junnare.pdf\r\n",
      "201_Jeana_Choi.pdf\r\n",
      "203_Tessa_Patapoutian.pdf\r\n",
      "204_Jenny_Baran.pdf\r\n",
      "205_Isabelle_Tjokrosetio.pdf\r\n",
      "208_Julia_Seward.pdf\r\n",
      "209_Adi_Geva.pdf\r\n",
      "20_Leila_Eshghi.pdf\r\n",
      "211_Linh_Nguyen.pdf\r\n",
      "212_Noor_Aftab.pdf\r\n",
      "213_Paula_Wambui.pdf\r\n",
      "214_Fnu_Swati.pdf\r\n",
      "217_Lin-ye_Kaye.pdf\r\n",
      "219_Anna_Mun.pdf\r\n",
      "21_Arushi_Ahmed.pdf\r\n",
      "220_Ishika_Jain.pdf\r\n",
      "221_Shailaja_Sampat.pdf\r\n",
      "222_Cici_Chen.pdf\r\n",
      "223_Aminata_Jaiteh.pdf\r\n",
      "224_Amaryce_Osorio.pdf\r\n",
      "225_Kathleen_Doviken.pdf\r\n",
      "226_Zeynep_Kahraman.pdf\r\n",
      "228_Sarah_Weinstein.pdf\r\n",
      "229_Zarrin_Ali.pdf\r\n",
      "230_Daria_Garkavtseva.pdf\r\n",
      "232_Mariah_Buccat.pdf\r\n",
      "234_Johanne_Antoine.pdf\r\n",
      "237_Iti_Shree.pdf\r\n",
      "238_Vinisha_Dhuleshia.pdf\r\n",
      "239_Camilla_Miquelina.pdf\r\n",
      "23_Tarang_Mittal.pdf\r\n",
      "240_Cynthia_Aroke.pdf\r\n",
      "241_Jingqiao_Huang.pdf\r\n",
      "243_Michelle_Tsai_Gomez.pdf\r\n",
      "244_Seetha_Chock.pdf\r\n",
      "245_Claudia_Olson.pdf\r\n",
      "246_Prachi_Modi.pdf\r\n",
      "247_Ashley_Farrell.pdf\r\n",
      "248_Maeve_Newman.pdf\r\n",
      "249_Janice_He.pdf\r\n",
      "24_Frances_Mulligan.pdf\r\n",
      "250_Paulaine_Goldsmith.pdf\r\n",
      "251_Lillian_Grassin-Drake.pdf\r\n",
      "252_Violette_Similien.pdf\r\n",
      "253_Samantha_Troxell.pdf\r\n",
      "254_Jeini_Mejia_Molina.pdf\r\n",
      "255_Pegah_Rashidnia.pdf\r\n",
      "256_Britney_Bourassa.pdf\r\n",
      "257_Anushree_Goswami.pdf\r\n",
      "258_Neamat_Sabry.pdf\r\n",
      "259_Sandra_Zhen.pdf\r\n",
      "25_Alicia_Ma_Shum.pdf\r\n",
      "260_Jingyu_Chen.pdf\r\n",
      "263_Raveena_Mewani.pdf\r\n",
      "264_Ana_Verma.pdf\r\n",
      "266_Dorcas_Ujiji.pdf\r\n",
      "267_Tina_Liu.pdf\r\n",
      "268_Sophie_Li.pdf\r\n",
      "269_Eunice_Kim.pdf\r\n",
      "271_Sarah_Jochum.pdf\r\n",
      "274_Angel_Huang.pdf\r\n",
      "275_Faustina_Owusu.pdf\r\n",
      "276_Shruti_Jain.pdf\r\n",
      "277_Tina_Su.pdf\r\n",
      "27_Michaela_Digan.pdf\r\n",
      "280_Alexis_Kilayko.pdf\r\n",
      "284_Hoang_Ho.pdf\r\n",
      "285_Alexandra_Appel.pdf\r\n",
      "287_Zoe_Simonson.pdf\r\n",
      "289_Celia_Lewis.pdf\r\n",
      "28_Jamaya_Smith.pdf\r\n",
      "290_Elita_lobo.pdf\r\n",
      "291_Kriti_Myer.pdf\r\n",
      "293_Roshni_Biswas.pdf\r\n",
      "294_Janki_Chaudhari.pdf\r\n",
      "297_Rose_Sheehan.pdf\r\n",
      "298_Ramona_Sar.pdf\r\n",
      "29_Lauren_Kenyon.pdf\r\n",
      "2_Test_Account.pdf\r\n",
      "300_SNEH_KOUL.pdf\r\n",
      "301_Jocelyn_Zhu.pdf\r\n",
      "302_anubha_Thandlay.pdf\r\n",
      "303_Jennifer_Lai.pdf\r\n",
      "305_Shanice_Smith.pdf\r\n",
      "306_Koyena_Pal.pdf\r\n",
      "307_Rosmery_Izaguirre.pdf\r\n",
      "309_Rebecca_Wang.pdf\r\n",
      "310_Ganishka_Bakshi.pdf\r\n",
      "311_Matthew_Smith.pdf\r\n",
      "312_Yi_ling_Pang.pdf\r\n",
      "314_Chen_Jin.pdf\r\n",
      "31_Alicia_Bochnak.pdf\r\n",
      "347_Karina_Popovich.pdf\r\n",
      "348_Helena_Merk.pdf\r\n",
      "349_Stephanie_Murphy.pdf\r\n",
      "350_Jordan_Graves.pdf\r\n",
      "352_Linh_Dang.pdf\r\n",
      "353_Jessica_Brito.pdf\r\n",
      "355_Nashshaba_Nawaz.pdf\r\n",
      "356_Christopher_Lianides.pdf\r\n",
      "361_Shababa_Kamreen.pdf\r\n",
      "367_Ramya_Sarma.pdf\r\n",
      "368_Micah_Reid.pdf\r\n",
      "369_Cynthia_Hester.pdf\r\n",
      "370_Vicky_McDermott.pdf\r\n",
      "374_Veena_Chittamuri.pdf\r\n",
      "375_Kirsten_Lydic.pdf\r\n",
      "376_Ianka_Mitchell-Conway.pdf\r\n",
      "378_Gautam_Worah.pdf\r\n",
      "380_Jessica_Zhou.pdf\r\n",
      "381_Angela_Bonsol.pdf\r\n",
      "382_KieuLy_Nguyen.pdf\r\n",
      "383_Veena_Lagare.pdf\r\n",
      "384_Bissenbay_Dauletbayev.pdf\r\n",
      "385_Mahima_Beltur.pdf\r\n",
      "386_Ashvita_Vasireddy.pdf\r\n",
      "387_Shreya_Ravi.pdf\r\n",
      "388_Radhika_Hegde.pdf\r\n",
      "389_Sage_Mahannah.pdf\r\n",
      "391_Katherine_Manser.pdf\r\n",
      "392_Soumya_Duriseti.pdf\r\n",
      "393_Emily_Daubenspeck.pdf\r\n",
      "394_Michelle_Mantilla.pdf\r\n",
      "395_Sharon_Liu.pdf\r\n",
      "396_Mira_Kasari.pdf\r\n",
      "397_Cat_Nguyen.pdf\r\n",
      "398_eman_alaa.pdf\r\n",
      "399_Arianna_Kazemi.pdf\r\n",
      "39_Chelsea_Perez.pdf\r\n",
      "402_Santhosh_Ramachandran.pdf\r\n",
      "403_Allison_Basore.pdf\r\n",
      "404_Stepha_Mills.pdf\r\n",
      "405_Syrine_Matoussi.pdf\r\n",
      "406_Manali_Palwankar.pdf\r\n",
      "407_Ibiyemisi_Gbenebor.pdf\r\n",
      "409_Dan_Nguyen.pdf\r\n",
      "411_Audrey_Lee.pdf\r\n",
      "412_Jenna_Lau.pdf\r\n",
      "413_Isabel_Paine.pdf\r\n",
      "414_Caroline_Liu.pdf\r\n",
      "415_Amelia_Paine.pdf\r\n",
      "416_Ayushi_Gupta.pdf\r\n",
      "418_Pavithra_Nagarajan.pdf\r\n",
      "419_Shruti_Nanda.pdf\r\n",
      "420_Tiffany_Yeung.pdf\r\n",
      "421_Colleen_Lam.pdf\r\n",
      "422_Amee_Trivedi.pdf\r\n",
      "423_Yasmeen_Mekky.pdf\r\n",
      "425_Nancy_Agarwal.pdf\r\n",
      "426_Anisha_Pai.pdf\r\n",
      "427_Semirah_Dolan.pdf\r\n",
      "428_Amy_Nguyen.pdf\r\n",
      "429_Denise_Nava.pdf\r\n",
      "42_Vani_Korepu.pdf\r\n",
      "430_FOROUGH_NEZAM.pdf\r\n",
      "431_Meghan_Healey.pdf\r\n",
      "433_Anna-Maria_Miller.pdf\r\n",
      "434_Tiffany_Rzepka.pdf\r\n",
      "435_Yixue_Wang.pdf\r\n",
      "436_Jackie_Kim.pdf\r\n",
      "437_Sejal_Agarwal.pdf\r\n",
      "438_Sajeda_Mokbel.pdf\r\n",
      "439_Shyla_Gangwar.pdf\r\n",
      "43_Camilla_Miquelina.pdf\r\n",
      "440_Nicole_Arabie.pdf\r\n",
      "442_Isabelle_Levy.pdf\r\n",
      "444_Sabrina_Choa.pdf\r\n",
      "446_Miracle_Olatunji.pdf\r\n",
      "447_Deesha_Shah.pdf\r\n",
      "448_Julia_Hines.pdf\r\n",
      "449_Jessica_Jin.pdf\r\n",
      "451_Tabara_Nosiba.pdf\r\n",
      "452_Mengyao_Zhu.pdf\r\n",
      "453_Debankita_Basu.pdf\r\n",
      "455_Rachael_Drinker.pdf\r\n",
      "456_Ellen_Dalen.pdf\r\n",
      "458_Lucy_Newman.pdf\r\n",
      "459_Liza_Bialik.pdf\r\n",
      "460_Amberly_Lerner.pdf\r\n",
      "461_Mary_Grace_Neville.pdf\r\n",
      "462_Sree_Lakshmi_Addepalli.pdf\r\n",
      "463_Nanako_Chung.pdf\r\n",
      "464_Himanshi_Dubey.pdf\r\n",
      "465_Poornima_Haridas.pdf\r\n",
      "466_Emily_Huang.pdf\r\n",
      "467_Meng_Li.pdf\r\n",
      "468_Anvita_Marla.pdf\r\n",
      "469_Iqra_Shaikh.pdf\r\n",
      "46_Emily_Rhyu.pdf\r\n",
      "470_Tejal_Lotlikar.pdf\r\n",
      "472_Carrie_Lamb.pdf\r\n",
      "473_Vanessa_Sun.pdf\r\n",
      "474_Poorva_Sonparote.pdf\r\n",
      "475_Gabriella_McLellan.pdf\r\n",
      "476_Caroline_Chen.pdf\r\n",
      "477_Vanessa_Martinez.pdf\r\n",
      "478_Ishita_Kumar.pdf\r\n",
      "479_Luisely_Doza.pdf\r\n",
      "47_Amritha_Anup.pdf\r\n",
      "480_Isabel_Chesla.pdf\r\n",
      "481_Liv_Hovey.pdf\r\n",
      "482_Qing_Zhao.pdf\r\n",
      "483_Camilla_Satte.pdf\r\n",
      "484_Hui_Duan.pdf\r\n",
      "486_Tiffany_Tang.pdf\r\n",
      "487_Sarah_Widrow.pdf\r\n",
      "488_Pazit_Schrecker.pdf\r\n",
      "489_Lara_Kollokian.pdf\r\n",
      "490_Stephanie_Lieu.pdf\r\n",
      "492_Sahar_Khan.pdf\r\n",
      "493_Luna_Ruiz.pdf\r\n",
      "494_Amelia_Tran.pdf\r\n",
      "495_Isabella_Ting.pdf\r\n",
      "496_Daria_Manea.pdf\r\n",
      "49_Jade_Saint-Paul.pdf\r\n",
      "500_Maham_Imtiaz.pdf\r\n",
      "501_Alondra_Esparza.pdf\r\n",
      "502_Wendy_Cheng.pdf\r\n",
      "503_Julia_Vorobeychik.pdf\r\n",
      "505_Radhika_Nikam.pdf\r\n",
      "507_Kelsie_McAllister.pdf\r\n",
      "508_Prashanthi_Kanniappan_Murthy.pdf\r\n",
      "510_Minh_Tran.pdf\r\n",
      "511_Kallan_Angus.pdf\r\n",
      "512_Mia_Celik.pdf\r\n",
      "513_Catherine_Simonds.pdf\r\n",
      "514_Shruti_Jalan.pdf\r\n",
      "515_Hoa_Duong.pdf\r\n",
      "518_Brittany_Laginhas.pdf\r\n",
      "519_Colleen_Foley.pdf\r\n",
      "51_Ruby_Ramsay.pdf\r\n",
      "520_Kavya_Sree_Bhagavatula.pdf\r\n",
      "521_Marly_Dure.pdf\r\n",
      "522_Caitlin_Ong.pdf\r\n",
      "524_Buyannemekh_Munkhbat.pdf\r\n",
      "525_Jaclyn_Acquaviva.pdf\r\n",
      "526_nikhitha_bonthala.pdf\r\n",
      "527_shengnan_ke.pdf\r\n",
      "528_Mary_Gerace.pdf\r\n",
      "529_Caitlin_Brightman.pdf\r\n",
      "530_Bryhannah_Young.pdf\r\n",
      "531_Brissa_Ramos.pdf\r\n",
      "532_Drishti_Agarwala.pdf\r\n",
      "533_Rebecca_Louisthelmy.pdf\r\n",
      "534_Li_Wang.pdf\r\n",
      "535_Rinija_Raja.pdf\r\n",
      "536_Margarita_Gubanova.pdf\r\n",
      "537_Hailey_Pensky.pdf\r\n",
      "538_Anjana_Chandra.pdf\r\n",
      "53_Ibtihal_Aboussad.pdf\r\n",
      "540_Ariana_Adames.pdf\r\n",
      "541_Amanda_Durfee.pdf\r\n",
      "542_SHAN_JIANG.pdf\r\n",
      "543_Hailey_Goldsmith.pdf\r\n",
      "545_Grace_Melcher.pdf\r\n",
      "546_Saranya_Krishnakumar.pdf\r\n",
      "547_Megan_Dion.pdf\r\n",
      "548_Angelina_Shalupova.pdf\r\n",
      "54_Yannan_Tuo.pdf\r\n",
      "550_Melissa_Lin.pdf\r\n",
      "551_Tariq_Ahmad.pdf\r\n",
      "552_Yashika_Issrani.pdf\r\n",
      "553_Ludmyla_Almeida.pdf\r\n",
      "554_Anjali_Patel.pdf\r\n",
      "555_Tahsina_Saosun.pdf\r\n",
      "556_Areeta_Wong.pdf\r\n",
      "557_Hia_Ghosh.pdf\r\n",
      "55_Lauren_Machere.pdf\r\n",
      "560_Stephanie_Doan.pdf\r\n",
      "561_Hannah_Lerner.pdf\r\n",
      "562_Margaret_Hardin.pdf\r\n",
      "564_Sierra_Jiang.pdf\r\n",
      "565_Marlaina_Fulmer.pdf\r\n",
      "566_Autumn_Phaneuf.pdf\r\n",
      "568_Jackie_Rowe.pdf\r\n",
      "569_Victoria_Caruso.pdf\r\n",
      "56_Julia_Kazmer.pdf\r\n",
      "570_Jacqueline_Lagasse.pdf\r\n",
      "573_Victoire_Beaufils.pdf\r\n",
      "576_Erin_Chung.pdf\r\n",
      "577_Devanshi_Udeshi.pdf\r\n",
      "578_Ha_Do.pdf\r\n",
      "58_Jessica_Fang.pdf\r\n",
      "611_Sree_Gowri_Addepalli.pdf\r\n",
      "612_Testing_Testing.pdf\r\n",
      "613_Juhi_Shah.pdf\r\n",
      "614_Shelby_Allen.pdf\r\n",
      "615_Emily_Crook.pdf\r\n",
      "616_Emilyann_Nault.pdf\r\n",
      "617_Shreya_Pandey.pdf\r\n",
      "618_Aishwarya_Budhkar.pdf\r\n",
      "619_Tracy_Dong.pdf\r\n",
      "61_Sophia_Berger.pdf\r\n",
      "620_Ah_Rim_Chung.pdf\r\n",
      "621_Christine_Chern.pdf\r\n",
      "622_Jetasri_Gupta.pdf\r\n",
      "626_Zsozso_Biegl.pdf\r\n",
      "628_Kathleen_Quinn.pdf\r\n",
      "629_Testing_Testing.pdf\r\n",
      "630_Sarah_Manlove.pdf\r\n",
      "632_Thanathorn_Sukprasert.pdf\r\n",
      "634_Nikitha_Preetham.pdf\r\n",
      "635_Chinh_Do.pdf\r\n",
      "636_Kiera_Perryman.pdf\r\n",
      "637_Srisuma_Movva.pdf\r\n",
      "638_Soumya_Paliwal.pdf\r\n",
      "639_Tram_Nguyen.pdf\r\n",
      "63_Ananya_Ganesh.pdf\r\n",
      "640_Kate_Newcomer.pdf\r\n",
      "641_Pooja_Hingorany.pdf\r\n",
      "644_Kyla_Rafferty.pdf\r\n",
      "645_Shaina_Peters.pdf\r\n",
      "646_Chianna_Cohen.pdf\r\n",
      "648_Holly_Wagner.pdf\r\n",
      "651_Mariah_Rucker.pdf\r\n",
      "652_Julia_Tseng.pdf\r\n",
      "654_Swayambita_Das.pdf\r\n",
      "655_Katelyn_Robertson.pdf\r\n",
      "657_Isha_Teredsai.pdf\r\n",
      "659_Sofya_Chow.pdf\r\n",
      "660_Angela_Maliakal.pdf\r\n",
      "662_Jenna_Melvin.pdf\r\n",
      "66_Colette_Basiliere.pdf\r\n",
      "68_Spenser_Kutney.pdf\r\n",
      "697_Miffy_Chen.pdf\r\n",
      "698_Gillian_Yue.pdf\r\n",
      "699_Ivana_Moore.pdf\r\n",
      "69_Emma_Fiore.pdf\r\n",
      "700_Taylor_Drinker.pdf\r\n",
      "701_Pauline_Gitau.pdf\r\n",
      "703_Julie_Washek.pdf\r\n",
      "705_Jenni_Norell.pdf\r\n",
      "706_Arshitha_Basavaraj.pdf\r\n",
      "707_Helaine_Hall.pdf\r\n",
      "708_Sophia_Wang.pdf\r\n",
      "709_Elisabetta_Gabriele.pdf\r\n",
      "70_Christy_Morin.pdf\r\n",
      "710_Claire_Sun.pdf\r\n",
      "711_Danielle_Mullan.pdf\r\n",
      "712_Sushmita_Chaudhari.pdf\r\n",
      "715_Rachel_Tham.pdf\r\n",
      "716_Kate_Spencer.pdf\r\n",
      "717_Sharon_Lin.pdf\r\n",
      "719_Franziska_Bright.pdf\r\n",
      "71_Sakshi_Gera.pdf\r\n",
      "720_Aidan_Fitzgerald.pdf\r\n",
      "722_Megan_Xie.pdf\r\n",
      "723_Katherin_Solis.pdf\r\n",
      "726_Iman_Anwarzai.pdf\r\n",
      "727_erin_curley.pdf\r\n",
      "728_Krista_Cepkauskas.pdf\r\n",
      "729_Priscilla_Saarah.pdf\r\n",
      "731_Suzen_Fylke.pdf\r\n",
      "732_Sohini_Kar.pdf\r\n",
      "733_Zhiyuan_Jia.pdf\r\n",
      "734_Lisa_Chan.pdf\r\n",
      "735_Luiza_Menezes.pdf\r\n",
      "736_Mindy_Zhang.pdf\r\n",
      "737_Isabelle_Nguyen-Phuoc.pdf\r\n",
      "738_Emai_Lai.pdf\r\n",
      "73_Yi-Pei_Chen.pdf\r\n",
      "742_Noureddine_Metourni.pdf\r\n",
      "744_Shirlyn_Prabahar.pdf\r\n",
      "745_Betty_Shen.pdf\r\n",
      "747_Anara_Yegizbayeva.pdf\r\n",
      "748_Lance_Margarett_Panes.pdf\r\n",
      "749_Jessica_Cheng.pdf\r\n",
      "74_Anjali_Devakumar.pdf\r\n",
      "750_Hue-Anh_Huynh.pdf\r\n",
      "751_Hana_Basheer.pdf\r\n",
      "752_Audrey_Lai.pdf\r\n",
      "753_Vaidehi_Thete.pdf\r\n",
      "754_Tanya_Pathak.pdf\r\n",
      "755_Preksha_Mutha.pdf\r\n",
      "756_Cara_Richardson.pdf\r\n",
      "759_Neha_Hulkund.pdf\r\n",
      "760_AJ_Asamoah.pdf\r\n",
      "761_Kendall_Wong.pdf\r\n",
      "763_Andrea_Joshua.pdf\r\n",
      "764_Soniya_Chawla.pdf\r\n",
      "765_Nidhi_Kargathra.pdf\r\n",
      "766_Rebecca_Malcolm.pdf\r\n",
      "767_Joyce_Huang.pdf\r\n",
      "769_Justin_Armstrong.pdf\r\n",
      "772_Nancy_Faria.pdf\r\n",
      "773_Apoorva_Karpurapu.pdf\r\n",
      "774_Kiera_Lewis.pdf\r\n",
      "776_Sushmita_Chaudhari.pdf\r\n",
      "777_Chianna_Cohen.pdf\r\n",
      "778_Akriti_Bhat.pdf\r\n",
      "779_Jun-Ru_Anderson.pdf\r\n",
      "77_Sophia_Chang.pdf\r\n",
      "780_Katrina_Greene.pdf\r\n",
      "782_Shanna_Chow.pdf\r\n",
      "787_siyang_tang.pdf\r\n",
      "788_Chineme_Ezema.pdf\r\n",
      "789_Jessica_Keast.pdf\r\n",
      "78_Ashira_Mawji.pdf\r\n",
      "790_Emily_Fritzman.pdf\r\n",
      "791_Emily_Dzwil.pdf\r\n",
      "792_Shivangi_Singh.pdf\r\n",
      "795_Arunika_Dhal.pdf\r\n",
      "796_Eula_Zhong.pdf\r\n",
      "797_Shania_Joseph.pdf\r\n",
      "798_Ramisa_Salsabil.pdf\r\n",
      "799_Dhara_Patel.pdf\r\n",
      "79_Akshita_Bhagia.pdf\r\n",
      "800_Sai_Pavitra_Madhurima_Dangati.pdf\r\n",
      "801_Christine_Taing.pdf\r\n",
      "803_yarlin_lopez.pdf\r\n",
      "804_Kaylien_Phan.pdf\r\n",
      "806_Eva_Tsimboukis.pdf\r\n",
      "808_Keccy_Zhang.pdf\r\n",
      "810_Charlotte_Nickerson.pdf\r\n",
      "811_Eliza_Huang.pdf\r\n",
      "813_Ke_Er_Liu.pdf\r\n",
      "814_Valentina_Moreno.pdf\r\n",
      "815_Maggie_Zhang.pdf\r\n",
      "816_Hannah_Baker.pdf\r\n",
      "817_Haoqin_Liang.pdf\r\n",
      "819_Alina_Valdez.pdf\r\n",
      "820_Eilat_Tabak.pdf\r\n",
      "821_Aishwarya_Sudhakar.pdf\r\n",
      "822_xiaoxue_lou.pdf\r\n",
      "823_Ly_Bui.pdf\r\n",
      "825_Nisarga_Patil.pdf\r\n",
      "827_Qianhui_Rong.pdf\r\n",
      "828_Annie_Chi.pdf\r\n",
      "829_Li_Hong.pdf\r\n",
      "830_Elise_Symmes.pdf\r\n",
      "832_Catherine_Holmes.pdf\r\n",
      "833_Srideepika_Jayaraman.pdf\r\n",
      "834_Twinkle_Tanna.pdf\r\n",
      "835_Pratheba_Selvaraju.pdf\r\n",
      "836_Lesley_Zheng.pdf\r\n",
      "837_Nikita_Jaiman.pdf\r\n",
      "838_Olivia_Ringham.pdf\r\n",
      "839_Lynn_Li.pdf\r\n",
      "83_Ji_Ye.pdf\r\n",
      "840_Angela_McNamara.pdf\r\n",
      "841_rashi_chauhan.pdf\r\n",
      "844_Lizette_Carpenter.pdf\r\n",
      "845_Megha_Shah.pdf\r\n",
      "846_Solana_Martinez.pdf\r\n",
      "847_yichun_Wang.pdf\r\n",
      "848_Kelly_Zeng.pdf\r\n",
      "849_Britnay_Beaudry.pdf\r\n",
      "84_Regina_Rex.pdf\r\n",
      "850_Megan_Alves.pdf\r\n",
      "851_Chloe_Zhang.pdf\r\n",
      "852_Neyissa_Exilus.pdf\r\n",
      "855_Emily_Li.pdf\r\n",
      "858_Rebecca_Schaefer.pdf\r\n",
      "85_Jhinnie_Shome.pdf\r\n",
      "860_Shiva_Ramezani.pdf\r\n",
      "861_Sam_Reardon.pdf\r\n",
      "862_Rachael_Enfield.pdf\r\n",
      "863_Vy_Doan.pdf\r\n",
      "864_Amelia_Vega.pdf\r\n",
      "865_Jie_Wang.pdf\r\n",
      "866_Joelle_Perez.pdf\r\n",
      "868_Annapurna_Jagasia.pdf\r\n",
      "869_Diana_Silva.pdf\r\n",
      "86_Unwana_Abasiurua.pdf\r\n",
      "870_Evangeline_Yuan.pdf\r\n",
      "871_Monica_Munnangi.pdf\r\n",
      "875_Karina_Lieb.pdf\r\n",
      "877_Rae_Maszer.pdf\r\n",
      "879_Rutuja_Pai.pdf\r\n",
      "880_Violet_Blue.pdf\r\n",
      "882_Sara_Dee.pdf\r\n",
      "883_Fusheng_Yang.pdf\r\n",
      "884_Yijing_He.pdf\r\n",
      "886_Memphis_Washington.pdf\r\n",
      "887_Ishita_Patel.pdf\r\n",
      "888_Sofia_Lis.pdf\r\n",
      "889_Emily_Zhong.pdf\r\n",
      "891_Yiran_Qi.pdf\r\n",
      "892_Nadia_L_Bahy.pdf\r\n",
      "893_Sofia_Marchetti.pdf\r\n",
      "894_Chimene_Minshew.pdf\r\n",
      "896_Chaitali_Ladikkar.pdf\r\n",
      "897_HUIYUN_PENG.pdf\r\n",
      "898_Emily_Dankiewicz.pdf\r\n",
      "899_Elsa_Kieffer.pdf\r\n",
      "8_Katie_House.pdf\r\n",
      "900_Anne-Gail_Moreland.pdf\r\n",
      "901_Mariama_Jaiteh.pdf\r\n",
      "902_Sanskruti_Shah.pdf\r\n",
      "903_Morgan_DiPilla.pdf\r\n",
      "907_Nasana_Bajracharya.pdf\r\n",
      "909_Merve_Durak.pdf\r\n",
      "911_Elisabeth_Eigerman.pdf\r\n",
      "912_Lynn_Zhu.pdf\r\n",
      "913_Jiaming_Cui.pdf\r\n",
      "914_Andrea_Mayoral.pdf\r\n",
      "915_Celia_Neveu.pdf\r\n",
      "916_Aayushi_Berry.pdf\r\n",
      "917_Jeng-Yu_Chou.pdf\r\n",
      "918_Sia_Ratan.pdf\r\n",
      "919_Marium_Tapal.pdf\r\n",
      "91_Amanda_Neelapriyantha.pdf\r\n",
      "920_hayley_green.pdf\r\n",
      "921_Michelle_Chen.pdf\r\n",
      "923_Sravya_Avadhuta.pdf\r\n",
      "924_Ayako_Ohara.pdf\r\n",
      "926_Nicole_Saleh.pdf\r\n",
      "928_Basira_Daqiq.pdf\r\n",
      "929_Catlinh_Le.pdf\r\n",
      "92_Eugene_Choi.pdf\r\n",
      "930_Camille_Godbout-Chouinard.pdf\r\n",
      "931_Jiaxuan_Ji.pdf\r\n",
      "932_Neha_Potturu.pdf\r\n",
      "934_Vincent_Chov.pdf\r\n",
      "937_Seyedeh_Arta_Razavi.pdf\r\n",
      "938_Raveena_Dookhan.pdf\r\n",
      "941_Ishita_Ankit.pdf\r\n",
      "942_Sophia_Chen.pdf\r\n",
      "943_Asia_Shell.pdf\r\n",
      "944_Yvonne_Lok.pdf\r\n",
      "945_Maya_Watanabe.pdf\r\n",
      "946_Sarah_Ke.pdf\r\n",
      "947_Ava_Bartolome.pdf\r\n",
      "948_Yayun_Fan.pdf\r\n",
      "949_Yian_Dimg.pdf\r\n",
      "950_Aakanksha_Duggal.pdf\r\n",
      "951_Vaishnavi_Kommaraju.pdf\r\n",
      "952_Rheeya_Uppaal.pdf\r\n",
      "953_Jennifer_Kahn.pdf\r\n",
      "954_Amrita_Reddy.pdf\r\n",
      "956_Pooja_Kundaje.pdf\r\n",
      "958_Fatema_Uddin.pdf\r\n",
      "95_Samantah_Rydzewski.pdf\r\n",
      "961_Chen_Xie.pdf\r\n",
      "962_Pracheta_Boddavaram_Amaranath.pdf\r\n",
      "965_Ashley_Artwick.pdf\r\n",
      "967_Isabelle_Hodge.pdf\r\n",
      "968_Jieyu_Feng.pdf\r\n",
      "969_Jules_Kelly.pdf\r\n",
      "96_Akhila_Josyula.pdf\r\n",
      "970_Ashley_Hoang.pdf\r\n",
      "971_aayushi_baghel.pdf\r\n",
      "976_Calista_Amoah.pdf\r\n",
      "977_Spyridoula_Potamopoulou.pdf\r\n",
      "97_Victoria_Liu.pdf\r\n",
      "980_Rocio_Jaime.pdf\r\n",
      "98_Anushree_Jana.pdf\r\n",
      "996_Alexus_Wang.pdf\r\n",
      "9_Ishan_Khatri.pdf\r\n"
     ]
    }
   ],
   "source": [
    "# This line refreshes the known data in the resume folder\n",
    "!ls \"HackHer413_Resumes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mypath = \"HackHer413_Resumes\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HackHer413_Resumes/920_hayley_green.pdf\n",
      "HackHer413_Resumes/801_Christine_Taing.pdf\n",
      "HackHer413_Resumes/96_Akhila_Josyula.pdf\n",
      "HackHer413_Resumes/830_Elise_Symmes.pdf\n",
      "HackHer413_Resumes/284_Hoang_Ho.pdf\n",
      "HackHer413_Resumes/958_Fatema_Uddin.pdf\n",
      "HackHer413_Resumes/39_Chelsea_Perez.pdf\n",
      "HackHer413_Resumes/773_Apoorva_Karpurapu.pdf\n",
      "HackHer413_Resumes/246_Prachi_Modi.pdf\n",
      "HackHer413_Resumes/494_Amelia_Tran.pdf\n",
      "HackHer413_Resumes/492_Sahar_Khan.pdf\n",
      "HackHer413_Resumes/952_Rheeya_Uppaal.pdf\n",
      "HackHer413_Resumes/222_Cici_Chen.pdf\n",
      "HackHer413_Resumes/376_Ianka_Mitchell-Conway.pdf\n",
      "HackHer413_Resumes/223_Aminata_Jaiteh.pdf\n",
      "HackHer413_Resumes/914_Andrea_Mayoral.pdf\n",
      "HackHer413_Resumes/861_Sam_Reardon.pdf\n",
      "HackHer413_Resumes/412_Jenna_Lau.pdf\n",
      "HackHer413_Resumes/409_Dan_Nguyen.pdf\n",
      "HackHer413_Resumes/380_Jessica_Zhou.pdf\n",
      "HackHer413_Resumes/845_Megha_Shah.pdf\n",
      "HackHer413_Resumes/570_Jacqueline_Lagasse.pdf\n",
      "HackHer413_Resumes/795_Arunika_Dhal.pdf\n",
      "HackHer413_Resumes/744_Shirlyn_Prabahar.pdf\n",
      "HackHer413_Resumes/911_Elisabeth_Eigerman.pdf\n",
      "HackHer413_Resumes/268_Sophie_Li.pdf\n",
      "HackHer413_Resumes/74_Anjali_Devakumar.pdf\n",
      "HackHer413_Resumes/749_Jessica_Cheng.pdf\n",
      "HackHer413_Resumes/203_Tessa_Patapoutian.pdf\n",
      "HackHer413_Resumes/641_Pooja_Hingorany.pdf\n",
      "HackHer413_Resumes/467_Meng_Li.pdf\n",
      "HackHer413_Resumes/128_Sarah_Andraka.pdf\n",
      "HackHer413_Resumes/532_Drishti_Agarwala.pdf\n",
      "HackHer413_Resumes/415_Amelia_Paine.pdf\n",
      "HackHer413_Resumes/896_Chaitali_Ladikkar.pdf\n",
      "HackHer413_Resumes/846_Solana_Martinez.pdf\n",
      "HackHer413_Resumes/819_Alina_Valdez.pdf\n",
      "HackHer413_Resumes/557_Hia_Ghosh.pdf\n",
      "HackHer413_Resumes/361_Shababa_Kamreen.pdf\n",
      "HackHer413_Resumes/729_Priscilla_Saarah.pdf\n",
      "HackHer413_Resumes/552_Yashika_Issrani.pdf\n",
      "HackHer413_Resumes/761_Kendall_Wong.pdf\n",
      "HackHer413_Resumes/469_Iqra_Shaikh.pdf\n",
      "HackHer413_Resumes/143_amy_guo.pdf\n",
      "HackHer413_Resumes/759_Neha_Hulkund.pdf\n",
      "HackHer413_Resumes/916_Aayushi_Berry.pdf\n",
      "HackHer413_Resumes/622_Jetasri_Gupta.pdf\n",
      "HackHer413_Resumes/256_Britney_Bourassa.pdf\n",
      "HackHer413_Resumes/83_Ji_Ye.pdf\n",
      "HackHer413_Resumes/698_Gillian_Yue.pdf\n",
      "HackHer413_Resumes/716_Kate_Spencer.pdf\n",
      "HackHer413_Resumes/294_Janki_Chaudhari.pdf\n",
      "HackHer413_Resumes/473_Vanessa_Sun.pdf\n",
      "HackHer413_Resumes/192_Sophie_Vincent.pdf\n",
      "HackHer413_Resumes/765_Nidhi_Kargathra.pdf\n",
      "HackHer413_Resumes/397_Cat_Nguyen.pdf\n",
      "HackHer413_Resumes/731_Suzen_Fylke.pdf\n",
      "HackHer413_Resumes/907_Nasana_Bajracharya.pdf\n",
      "HackHer413_Resumes/954_Amrita_Reddy.pdf\n",
      "HackHer413_Resumes/923_Sravya_Avadhuta.pdf\n",
      "HackHer413_Resumes/727_erin_curley.pdf\n",
      "HackHer413_Resumes/277_Tina_Su.pdf\n",
      "HackHer413_Resumes/188_Diane_Tam.pdf\n",
      "HackHer413_Resumes/251_Lillian_Grassin-Drake.pdf\n",
      "HackHer413_Resumes/712_Sushmita_Chaudhari.pdf\n",
      "HackHer413_Resumes/834_Twinkle_Tanna.pdf\n",
      "HackHer413_Resumes/917_Jeng-Yu_Chou.pdf\n",
      "HackHer413_Resumes/98_Anushree_Jana.pdf\n",
      "HackHer413_Resumes/892_Nadia_L_Bahy.pdf\n",
      "HackHer413_Resumes/433_Anna-Maria_Miller.pdf\n",
      "HackHer413_Resumes/24_Frances_Mulligan.pdf\n",
      "HackHer413_Resumes/915_Celia_Neveu.pdf\n",
      "HackHer413_Resumes/398_eman_alaa.pdf\n",
      "HackHer413_Resumes/706_Arshitha_Basavaraj.pdf\n",
      "HackHer413_Resumes/464_Himanshi_Dubey.pdf\n",
      "HackHer413_Resumes/701_Pauline_Gitau.pdf\n",
      "HackHer413_Resumes/434_Tiffany_Rzepka.pdf\n",
      "HackHer413_Resumes/953_Jennifer_Kahn.pdf\n",
      "HackHer413_Resumes/644_Kyla_Rafferty.pdf\n",
      "HackHer413_Resumes/355_Nashshaba_Nawaz.pdf\n",
      "HackHer413_Resumes/944_Yvonne_Lok.pdf\n",
      "HackHer413_Resumes/943_Asia_Shell.pdf\n",
      "HackHer413_Resumes/531_Brissa_Ramos.pdf\n",
      "HackHer413_Resumes/414_Caroline_Liu.pdf\n",
      "HackHer413_Resumes/697_Miffy_Chen.pdf\n",
      "HackHer413_Resumes/190_Amanda_Murray.pdf\n",
      "HackHer413_Resumes/465_Poornima_Haridas.pdf\n",
      "HackHer413_Resumes/109_Paulina_Valdivieso.pdf\n",
      "HackHer413_Resumes/924_Ayako_Ohara.pdf\n",
      "HackHer413_Resumes/780_Katrina_Greene.pdf\n",
      "HackHer413_Resumes/971_aayushi_baghel.pdf\n",
      "HackHer413_Resumes/463_Nanako_Chung.pdf\n",
      "HackHer413_Resumes/837_Nikita_Jaiman.pdf\n",
      "HackHer413_Resumes/628_Kathleen_Quinn.pdf\n",
      "HackHer413_Resumes/947_Ava_Bartolome.pdf\n",
      "HackHer413_Resumes/260_Jingyu_Chen.pdf\n",
      "HackHer413_Resumes/21_Arushi_Ahmed.pdf\n",
      "HackHer413_Resumes/521_Marly_Dure.pdf\n",
      "HackHer413_Resumes/217_Lin-ye_Kaye.pdf\n",
      "HackHer413_Resumes/530_Bryhannah_Young.pdf\n",
      "HackHer413_Resumes/513_Catherine_Simonds.pdf\n",
      "HackHer413_Resumes/239_Camilla_Miquelina.pdf\n",
      "HackHer413_Resumes/708_Sophia_Wang.pdf\n",
      "HackHer413_Resumes/2_Test_Account.pdf\n",
      "HackHer413_Resumes/301_Jocelyn_Zhu.pdf\n",
      "HackHer413_Resumes/566_Autumn_Phaneuf.pdf\n",
      "HackHer413_Resumes/209_Adi_Geva.pdf\n",
      "HackHer413_Resumes/769_Justin_Armstrong.pdf\n",
      "HackHer413_Resumes/577_Devanshi_Udeshi.pdf\n",
      "HackHer413_Resumes/174_Caitlin_Pichette.pdf\n",
      "HackHer413_Resumes/615_Emily_Crook.pdf\n",
      "HackHer413_Resumes/309_Rebecca_Wang.pdf\n",
      "HackHer413_Resumes/132_Gabrielle_Newman.pdf\n",
      "HackHer413_Resumes/461_Mary_Grace_Neville.pdf\n",
      "HackHer413_Resumes/116_Laurel_Parsons.pdf\n",
      "HackHer413_Resumes/213_Paula_Wambui.pdf\n",
      "HackHer413_Resumes/723_Katherin_Solis.pdf\n",
      "HackHer413_Resumes/932_Neha_Potturu.pdf\n",
      "HackHer413_Resumes/848_Kelly_Zeng.pdf\n",
      "HackHer413_Resumes/766_Rebecca_Malcolm.pdf\n",
      "HackHer413_Resumes/200_Radhika_Junnare.pdf\n",
      "HackHer413_Resumes/243_Michelle_Tsai_Gomez.pdf\n",
      "HackHer413_Resumes/241_Jingqiao_Huang.pdf\n",
      "HackHer413_Resumes/381_Angela_Bonsol.pdf\n",
      "HackHer413_Resumes/386_Ashvita_Vasireddy.pdf\n",
      "HackHer413_Resumes/865_Jie_Wang.pdf\n",
      "HackHer413_Resumes/535_Rinija_Raja.pdf\n",
      "HackHer413_Resumes/162_Sreeya_Sai.pdf\n",
      "HackHer413_Resumes/423_Yasmeen_Mekky.pdf\n",
      "HackHer413_Resumes/808_Keccy_Zhang.pdf\n",
      "HackHer413_Resumes/720_Aidan_Fitzgerald.pdf\n",
      "HackHer413_Resumes/891_Yiran_Qi.pdf\n",
      "HackHer413_Resumes/561_Hannah_Lerner.pdf\n",
      "HackHer413_Resumes/619_Tracy_Dong.pdf\n",
      "HackHer413_Resumes/792_Shivangi_Singh.pdf\n",
      "HackHer413_Resumes/240_Cynthia_Aroke.pdf\n",
      "HackHer413_Resumes/835_Pratheba_Selvaraju.pdf\n",
      "HackHer413_Resumes/495_Isabella_Ting.pdf\n",
      "HackHer413_Resumes/77_Sophia_Chang.pdf\n",
      "HackHer413_Resumes/122_Jaime_Woolcock.pdf\n",
      "HackHer413_Resumes/510_Minh_Tran.pdf\n",
      "HackHer413_Resumes/436_Jackie_Kim.pdf\n",
      "HackHer413_Resumes/247_Ashley_Farrell.pdf\n",
      "HackHer413_Resumes/53_Ibtihal_Aboussad.pdf\n",
      "HackHer413_Resumes/28_Jamaya_Smith.pdf\n",
      "HackHer413_Resumes/374_Veena_Chittamuri.pdf\n",
      "HackHer413_Resumes/742_Noureddine_Metourni.pdf\n",
      "HackHer413_Resumes/55_Lauren_Machere.pdf\n",
      "HackHer413_Resumes/146_Emily_Earl.pdf\n",
      "HackHer413_Resumes/383_Veena_Lagare.pdf\n",
      "HackHer413_Resumes/950_Aakanksha_Duggal.pdf\n",
      "HackHer413_Resumes/249_Janice_He.pdf\n",
      "HackHer413_Resumes/111_Ester_Zhao.pdf\n",
      "HackHer413_Resumes/968_Jieyu_Feng.pdf\n",
      "HackHer413_Resumes/941_Ishita_Ankit.pdf\n",
      "HackHer413_Resumes/115_Chesta_Singh.pdf\n",
      "HackHer413_Resumes/404_Stepha_Mills.pdf\n",
      "HackHer413_Resumes/175_Colleen_Dunlap.pdf\n",
      "HackHer413_Resumes/447_Deesha_Shah.pdf\n",
      "HackHer413_Resumes/501_Alondra_Esparza.pdf\n",
      "HackHer413_Resumes/897_HUIYUN_PENG.pdf\n",
      "HackHer413_Resumes/949_Yian_Dimg.pdf\n",
      "HackHer413_Resumes/508_Prashanthi_Kanniappan_Murthy.pdf\n",
      "HackHer413_Resumes/131_Leah_Leshchinsky.pdf\n",
      "HackHer413_Resumes/420_Tiffany_Yeung.pdf\n",
      "HackHer413_Resumes/160_Elizabeth_Wang.pdf\n",
      "HackHer413_Resumes/305_Shanice_Smith.pdf\n",
      "HackHer413_Resumes/827_Qianhui_Rong.pdf\n",
      "HackHer413_Resumes/887_Ishita_Patel.pdf\n",
      "HackHer413_Resumes/616_Emilyann_Nault.pdf\n",
      "HackHer413_Resumes/543_Hailey_Goldsmith.pdf\n",
      "HackHer413_Resumes/31_Alicia_Bochnak.pdf\n",
      "HackHer413_Resumes/533_Rebecca_Louisthelmy.pdf\n",
      "HackHer413_Resumes/545_Grace_Melcher.pdf\n",
      "HackHer413_Resumes/29_Lauren_Kenyon.pdf\n",
      "HackHer413_Resumes/705_Jenni_Norell.pdf\n",
      "HackHer413_Resumes/541_Amanda_Durfee.pdf\n",
      "HackHer413_Resumes/56_Julia_Kazmer.pdf\n",
      "HackHer413_Resumes/804_Kaylien_Phan.pdf\n",
      "HackHer413_Resumes/611_Sree_Gowri_Addepalli.pdf\n",
      "HackHer413_Resumes/102_Narpavirajan_Venkatesan_Muralidharan.pdf\n",
      "HackHer413_Resumes/113_YuanYi_Chen.pdf\n",
      "HackHer413_Resumes/483_Camilla_Satte.pdf\n",
      "HackHer413_Resumes/369_Cynthia_Hester.pdf\n",
      "HackHer413_Resumes/474_Poorva_Sonparote.pdf\n",
      "HackHer413_Resumes/382_KieuLy_Nguyen.pdf\n",
      "HackHer413_Resumes/70_Christy_Morin.pdf\n",
      "HackHer413_Resumes/290_Elita_lobo.pdf\n",
      "HackHer413_Resumes/578_Ha_Do.pdf\n",
      "HackHer413_Resumes/271_Sarah_Jochum.pdf\n",
      "HackHer413_Resumes/205_Isabelle_Tjokrosetio.pdf\n",
      "HackHer413_Resumes/832_Catherine_Holmes.pdf\n",
      "HackHer413_Resumes/918_Sia_Ratan.pdf\n",
      "HackHer413_Resumes/977_Spyridoula_Potamopoulou.pdf\n",
      "HackHer413_Resumes/440_Nicole_Arabie.pdf\n",
      "HackHer413_Resumes/148_Yomna_Barakat.pdf\n",
      "HackHer413_Resumes/648_Holly_Wagner.pdf\n",
      "HackHer413_Resumes/356_Christopher_Lianides.pdf\n",
      "HackHer413_Resumes/91_Amanda_Neelapriyantha.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HackHer413_Resumes/534_Li_Wang.pdf\n",
      "HackHer413_Resumes/912_Lynn_Zhu.pdf\n",
      "HackHer413_Resumes/612_Testing_Testing.pdf\n",
      "HackHer413_Resumes/652_Julia_Tseng.pdf\n",
      "HackHer413_Resumes/47_Amritha_Anup.pdf\n",
      "HackHer413_Resumes/760_AJ_Asamoah.pdf\n",
      "HackHer413_Resumes/106_Khanh_Ngo.pdf\n",
      "HackHer413_Resumes/108_Saumya_Priya.pdf\n",
      "HackHer413_Resumes/919_Marium_Tapal.pdf\n",
      "HackHer413_Resumes/291_Kriti_Myer.pdf\n",
      "HackHer413_Resumes/711_Danielle_Mullan.pdf\n",
      "HackHer413_Resumes/112_Karen_Santamaria.pdf\n",
      "HackHer413_Resumes/27_Michaela_Digan.pdf\n",
      "HackHer413_Resumes/550_Melissa_Lin.pdf\n",
      "HackHer413_Resumes/426_Anisha_Pai.pdf\n",
      "HackHer413_Resumes/220_Ishika_Jain.pdf\n",
      "HackHer413_Resumes/180_Catherine_Yeo.pdf\n",
      "HackHer413_Resumes/875_Karina_Lieb.pdf\n",
      "HackHer413_Resumes/951_Vaishnavi_Kommaraju.pdf\n",
      "HackHer413_Resumes/158_Hea_Rim_Yoon.pdf\n",
      "HackHer413_Resumes/142_snehitha_Ramasahayam.pdf\n",
      "HackHer413_Resumes/462_Sree_Lakshmi_Addepalli.pdf\n",
      "HackHer413_Resumes/182_Sharon_Jiang.pdf\n",
      "HackHer413_Resumes/626_Zsozso_Biegl.pdf\n",
      "HackHer413_Resumes/961_Chen_Xie.pdf\n",
      "HackHer413_Resumes/816_Hannah_Baker.pdf\n",
      "HackHer413_Resumes/511_Kallan_Angus.pdf\n",
      "HackHer413_Resumes/389_Sage_Mahannah.pdf\n",
      "HackHer413_Resumes/126_Ophelia_Mendoza.pdf\n",
      "HackHer413_Resumes/232_Mariah_Buccat.pdf\n",
      "HackHer413_Resumes/229_Zarrin_Ali.pdf\n",
      "HackHer413_Resumes/9_Ishan_Khatri.pdf\n",
      "HackHer413_Resumes/645_Shaina_Peters.pdf\n",
      "HackHer413_Resumes/416_Ayushi_Gupta.pdf\n",
      "HackHer413_Resumes/452_Mengyao_Zhu.pdf\n",
      "HackHer413_Resumes/444_Sabrina_Choa.pdf\n",
      "HackHer413_Resumes/480_Isabel_Chesla.pdf\n",
      "HackHer413_Resumes/184_Dyuti_De.pdf\n",
      "HackHer413_Resumes/754_Tanya_Pathak.pdf\n",
      "HackHer413_Resumes/901_Mariama_Jaiteh.pdf\n",
      "HackHer413_Resumes/764_Soniya_Chawla.pdf\n",
      "HackHer413_Resumes/230_Daria_Garkavtseva.pdf\n",
      "HackHer413_Resumes/475_Gabriella_McLellan.pdf\n",
      "HackHer413_Resumes/368_Micah_Reid.pdf\n",
      "HackHer413_Resumes/393_Emily_Daubenspeck.pdf\n",
      "HackHer413_Resumes/635_Chinh_Do.pdf\n",
      "HackHer413_Resumes/446_Miracle_Olatunji.pdf\n",
      "HackHer413_Resumes/130_Mehak_Lodhi.pdf\n",
      "HackHer413_Resumes/902_Sanskruti_Shah.pdf\n",
      "HackHer413_Resumes/877_Rae_Maszer.pdf\n",
      "HackHer413_Resumes/717_Sharon_Lin.pdf\n",
      "HackHer413_Resumes/68_Spenser_Kutney.pdf\n",
      "HackHer413_Resumes/43_Camilla_Miquelina.pdf\n",
      "HackHer413_Resumes/970_Ashley_Hoang.pdf\n",
      "HackHer413_Resumes/655_Katelyn_Robertson.pdf\n",
      "HackHer413_Resumes/512_Mia_Celik.pdf\n",
      "HackHer413_Resumes/482_Qing_Zhao.pdf\n",
      "HackHer413_Resumes/481_Liv_Hovey.pdf\n",
      "HackHer413_Resumes/161_Dhruvi_Vora.pdf\n",
      "HackHer413_Resumes/868_Annapurna_Jagasia.pdf\n",
      "HackHer413_Resumes/777_Chianna_Cohen.pdf\n",
      "HackHer413_Resumes/195_Hannah_Fraser.pdf\n",
      "HackHer413_Resumes/212_Noor_Aftab.pdf\n",
      "HackHer413_Resumes/300_SNEH_KOUL.pdf\n",
      "HackHer413_Resumes/378_Gautam_Worah.pdf\n",
      "HackHer413_Resumes/726_Iman_Anwarzai.pdf\n",
      "HackHer413_Resumes/73_Yi-Pei_Chen.pdf\n",
      "HackHer413_Resumes/189_Gabriella_Lalli_Martins.pdf\n",
      "HackHer413_Resumes/312_Yi_ling_Pang.pdf\n",
      "HackHer413_Resumes/302_anubha_Thandlay.pdf\n",
      "HackHer413_Resumes/406_Manali_Palwankar.pdf\n",
      "HackHer413_Resumes/909_Merve_Durak.pdf\n",
      "HackHer413_Resumes/817_Haoqin_Liang.pdf\n",
      "HackHer413_Resumes/176_Afreen_Aliya.pdf\n",
      "HackHer413_Resumes/833_Srideepika_Jayaraman.pdf\n",
      "HackHer413_Resumes/849_Britnay_Beaudry.pdf\n",
      "HackHer413_Resumes/519_Colleen_Foley.pdf\n",
      "HackHer413_Resumes/419_Shruti_Nanda.pdf\n",
      "HackHer413_Resumes/285_Alexandra_Appel.pdf\n",
      "HackHer413_Resumes/772_Nancy_Faria.pdf\n",
      "HackHer413_Resumes/779_Jun-Ru_Anderson.pdf\n",
      "HackHer413_Resumes/407_Ibiyemisi_Gbenebor.pdf\n",
      "HackHer413_Resumes/945_Maya_Watanabe.pdf\n",
      "HackHer413_Resumes/140_Juliana_Lugg.pdf\n",
      "HackHer413_Resumes/942_Sophia_Chen.pdf\n",
      "HackHer413_Resumes/806_Eva_Tsimboukis.pdf\n",
      "HackHer413_Resumes/748_Lance_Margarett_Panes.pdf\n",
      "HackHer413_Resumes/840_Angela_McNamara.pdf\n",
      "HackHer413_Resumes/69_Emma_Fiore.pdf\n",
      "HackHer413_Resumes/921_Michelle_Chen.pdf\n",
      "HackHer413_Resumes/707_Helaine_Hall.pdf\n",
      "HackHer413_Resumes/700_Taylor_Drinker.pdf\n",
      "HackHer413_Resumes/858_Rebecca_Schaefer.pdf\n",
      "HackHer413_Resumes/477_Vanessa_Martinez.pdf\n",
      "HackHer413_Resumes/646_Chianna_Cohen.pdf\n",
      "HackHer413_Resumes/97_Victoria_Liu.pdf\n",
      "HackHer413_Resumes/709_Elisabetta_Gabriele.pdf\n",
      "HackHer413_Resumes/505_Radhika_Nikam.pdf\n",
      "HackHer413_Resumes/79_Akshita_Bhagia.pdf\n",
      "HackHer413_Resumes/168_Tremelle_Lester.pdf\n",
      "HackHer413_Resumes/546_Saranya_Krishnakumar.pdf\n",
      "HackHer413_Resumes/42_Vani_Korepu.pdf\n",
      "HackHer413_Resumes/767_Joyce_Huang.pdf\n",
      "HackHer413_Resumes/502_Wendy_Cheng.pdf\n",
      "HackHer413_Resumes/659_Sofya_Chow.pdf\n",
      "HackHer413_Resumes/310_Ganishka_Bakshi.pdf\n",
      "HackHer413_Resumes/297_Rose_Sheehan.pdf\n",
      "HackHer413_Resumes/883_Fusheng_Yang.pdf\n",
      "HackHer413_Resumes/8_Katie_House.pdf\n",
      "HackHer413_Resumes/825_Nisarga_Patil.pdf\n",
      "HackHer413_Resumes/448_Julia_Hines.pdf\n",
      "HackHer413_Resumes/177_Claire_Jordan.pdf\n",
      "HackHer413_Resumes/863_Vy_Doan.pdf\n",
      "HackHer413_Resumes/520_Kavya_Sree_Bhagavatula.pdf\n",
      "HackHer413_Resumes/719_Franziska_Bright.pdf\n",
      "HackHer413_Resumes/736_Mindy_Zhang.pdf\n",
      "HackHer413_Resumes/145_Sydney_Smith.pdf\n",
      "HackHer413_Resumes/618_Aishwarya_Budhkar.pdf\n",
      "HackHer413_Resumes/733_Zhiyuan_Jia.pdf\n",
      "HackHer413_Resumes/226_Zeynep_Kahraman.pdf\n",
      "HackHer413_Resumes/823_Ly_Bui.pdf\n",
      "HackHer413_Resumes/348_Helena_Merk.pdf\n",
      "HackHer413_Resumes/164_Riya_Dhar.pdf\n",
      "HackHer413_Resumes/860_Shiva_Ramezani.pdf\n",
      "HackHer413_Resumes/488_Pazit_Schrecker.pdf\n",
      "HackHer413_Resumes/847_yichun_Wang.pdf\n",
      "HackHer413_Resumes/884_Yijing_He.pdf\n",
      "HackHer413_Resumes/489_Lara_Kollokian.pdf\n",
      "HackHer413_Resumes/493_Luna_Ruiz.pdf\n",
      "HackHer413_Resumes/613_Juhi_Shah.pdf\n",
      "HackHer413_Resumes/437_Sejal_Agarwal.pdf\n",
      "HackHer413_Resumes/144_Laura_O_Malley.pdf\n",
      "HackHer413_Resumes/266_Dorcas_Ujiji.pdf\n",
      "HackHer413_Resumes/750_Hue-Anh_Huynh.pdf\n",
      "HackHer413_Resumes/124_Vedantika_Chaudhary.pdf\n",
      "HackHer413_Resumes/850_Megan_Alves.pdf\n",
      "HackHer413_Resumes/84_Regina_Rex.pdf\n",
      "HackHer413_Resumes/751_Hana_Basheer.pdf\n",
      "HackHer413_Resumes/153_Megan_Cruz.pdf\n",
      "HackHer413_Resumes/101_Devika_Chipalkatti.pdf\n",
      "HackHer413_Resumes/411_Audrey_Lee.pdf\n",
      "HackHer413_Resumes/197_Frankie_Rourke.pdf\n",
      "HackHer413_Resumes/252_Violette_Similien.pdf\n",
      "HackHer413_Resumes/276_Shruti_Jain.pdf\n",
      "HackHer413_Resumes/851_Chloe_Zhang.pdf\n",
      "HackHer413_Resumes/560_Stephanie_Doan.pdf\n",
      "HackHer413_Resumes/814_Valentina_Moreno.pdf\n",
      "HackHer413_Resumes/456_Ellen_Dalen.pdf\n",
      "HackHer413_Resumes/632_Thanathorn_Sukprasert.pdf\n",
      "HackHer413_Resumes/569_Victoria_Caruso.pdf\n",
      "HackHer413_Resumes/820_Eilat_Tabak.pdf\n",
      "HackHer413_Resumes/120_Nathacha_Almanzar.pdf\n",
      "HackHer413_Resumes/822_xiaoxue_lou.pdf\n",
      "HackHer413_Resumes/453_Debankita_Basu.pdf\n",
      "HackHer413_Resumes/636_Kiera_Perryman.pdf\n",
      "HackHer413_Resumes/948_Yayun_Fan.pdf\n",
      "HackHer413_Resumes/114_Joanna_Olson.pdf\n",
      "HackHer413_Resumes/353_Jessica_Brito.pdf\n",
      "HackHer413_Resumes/496_Daria_Manea.pdf\n",
      "HackHer413_Resumes/253_Samantha_Troxell.pdf\n",
      "HackHer413_Resumes/138_Grainne_Casey.pdf\n",
      "HackHer413_Resumes/136_Mary_Kate_Murtagh.pdf\n",
      "HackHer413_Resumes/734_Lisa_Chan.pdf\n",
      "HackHer413_Resumes/427_Semirah_Dolan.pdf\n",
      "HackHer413_Resumes/289_Celia_Lewis.pdf\n",
      "HackHer413_Resumes/527_shengnan_ke.pdf\n",
      "HackHer413_Resumes/166_Sabrina_Taylor.pdf\n",
      "HackHer413_Resumes/946_Sarah_Ke.pdf\n",
      "HackHer413_Resumes/899_Elsa_Kieffer.pdf\n",
      "HackHer413_Resumes/92_Eugene_Choi.pdf\n",
      "HackHer413_Resumes/829_Li_Hong.pdf\n",
      "HackHer413_Resumes/490_Stephanie_Lieu.pdf\n",
      "HackHer413_Resumes/798_Ramisa_Salsabil.pdf\n",
      "HackHer413_Resumes/928_Basira_Daqiq.pdf\n",
      "HackHer413_Resumes/538_Anjana_Chandra.pdf\n",
      "HackHer413_Resumes/862_Rachael_Enfield.pdf\n",
      "HackHer413_Resumes/54_Yannan_Tuo.pdf\n",
      "HackHer413_Resumes/191_Shreya_Parashare.pdf\n",
      "HackHer413_Resumes/870_Evangeline_Yuan.pdf\n",
      "HackHer413_Resumes/245_Claudia_Olson.pdf\n",
      "HackHer413_Resumes/105_Shirley_Xu.pdf\n",
      "HackHer413_Resumes/565_Marlaina_Fulmer.pdf\n",
      "HackHer413_Resumes/880_Violet_Blue.pdf\n",
      "HackHer413_Resumes/738_Emai_Lai.pdf\n",
      "HackHer413_Resumes/118_Ilika_Tripathi.pdf\n",
      "HackHer413_Resumes/449_Jessica_Jin.pdf\n",
      "HackHer413_Resumes/66_Colette_Basiliere.pdf\n",
      "HackHer413_Resumes/515_Hoa_Duong.pdf\n",
      "HackHer413_Resumes/430_FOROUGH_NEZAM.pdf\n",
      "HackHer413_Resumes/551_Tariq_Ahmad.pdf\n",
      "HackHer413_Resumes/898_Emily_Dankiewicz.pdf\n",
      "HackHer413_Resumes/529_Caitlin_Brightman.pdf\n",
      "HackHer413_Resumes/280_Alexis_Kilayko.pdf\n",
      "HackHer413_Resumes/468_Anvita_Marla.pdf\n",
      "HackHer413_Resumes/791_Emily_Dzwil.pdf\n",
      "HackHer413_Resumes/458_Lucy_Newman.pdf\n",
      "HackHer413_Resumes/763_Andrea_Joshua.pdf\n",
      "HackHer413_Resumes/790_Emily_Fritzman.pdf\n",
      "HackHer413_Resumes/451_Tabara_Nosiba.pdf\n",
      "HackHer413_Resumes/811_Eliza_Huang.pdf\n",
      "HackHer413_Resumes/303_Jennifer_Lai.pdf\n",
      "HackHer413_Resumes/639_Tram_Nguyen.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HackHer413_Resumes/287_Zoe_Simonson.pdf\n",
      "HackHer413_Resumes/500_Maham_Imtiaz.pdf\n",
      "HackHer413_Resumes/46_Emily_Rhyu.pdf\n",
      "HackHer413_Resumes/839_Lynn_Li.pdf\n",
      "HackHer413_Resumes/275_Faustina_Owusu.pdf\n",
      "HackHer413_Resumes/413_Isabel_Paine.pdf\n",
      "HackHer413_Resumes/258_Neamat_Sabry.pdf\n",
      "HackHer413_Resumes/194_Lauren_McKim.pdf\n",
      "HackHer413_Resumes/774_Kiera_Lewis.pdf\n",
      "HackHer413_Resumes/514_Shruti_Jalan.pdf\n",
      "HackHer413_Resumes/654_Swayambita_Das.pdf\n",
      "HackHer413_Resumes/78_Ashira_Mawji.pdf\n",
      "HackHer413_Resumes/442_Isabelle_Levy.pdf\n",
      "HackHer413_Resumes/349_Stephanie_Murphy.pdf\n",
      "HackHer413_Resumes/418_Pavithra_Nagarajan.pdf\n",
      "HackHer413_Resumes/576_Erin_Chung.pdf\n",
      "HackHer413_Resumes/813_Ke_Er_Liu.pdf\n",
      "HackHer413_Resumes/796_Eula_Zhong.pdf\n",
      "HackHer413_Resumes/855_Emily_Li.pdf\n",
      "HackHer413_Resumes/976_Calista_Amoah.pdf\n",
      "HackHer413_Resumes/237_Iti_Shree.pdf\n",
      "HackHer413_Resumes/844_Lizette_Carpenter.pdf\n",
      "HackHer413_Resumes/871_Monica_Munnangi.pdf\n",
      "HackHer413_Resumes/149_Danielle_Finestone.pdf\n",
      "HackHer413_Resumes/620_Ah_Rim_Chung.pdf\n",
      "HackHer413_Resumes/528_Mary_Gerace.pdf\n",
      "HackHer413_Resumes/171_Michela_Marchini.pdf\n",
      "HackHer413_Resumes/564_Sierra_Jiang.pdf\n",
      "HackHer413_Resumes/889_Emily_Zhong.pdf\n",
      "HackHer413_Resumes/61_Sophia_Berger.pdf\n",
      "HackHer413_Resumes/254_Jeini_Mejia_Molina.pdf\n",
      "HackHer413_Resumes/107_SREEPARNA_MUKHERJEE.pdf\n",
      "HackHer413_Resumes/269_Eunice_Kim.pdf\n",
      "HackHer413_Resumes/370_Vicky_McDermott.pdf\n",
      "HackHer413_Resumes/573_Victoire_Beaufils.pdf\n",
      "HackHer413_Resumes/487_Sarah_Widrow.pdf\n",
      "HackHer413_Resumes/466_Emily_Huang.pdf\n",
      "HackHer413_Resumes/864_Amelia_Vega.pdf\n",
      "HackHer413_Resumes/553_Ludmyla_Almeida.pdf\n",
      "HackHer413_Resumes/930_Camille_Godbout-Chouinard.pdf\n",
      "HackHer413_Resumes/228_Sarah_Weinstein.pdf\n",
      "HackHer413_Resumes/391_Katherine_Manser.pdf\n",
      "HackHer413_Resumes/250_Paulaine_Goldsmith.pdf\n",
      "HackHer413_Resumes/980_Rocio_Jaime.pdf\n",
      "HackHer413_Resumes/838_Olivia_Ringham.pdf\n",
      "HackHer413_Resumes/866_Joelle_Perez.pdf\n",
      "HackHer413_Resumes/797_Shania_Joseph.pdf\n",
      "HackHer413_Resumes/298_Ramona_Sar.pdf\n",
      "HackHer413_Resumes/547_Megan_Dion.pdf\n",
      "HackHer413_Resumes/204_Jenny_Baran.pdf\n",
      "HackHer413_Resumes/405_Syrine_Matoussi.pdf\n",
      "HackHer413_Resumes/20_Leila_Eshghi.pdf\n",
      "HackHer413_Resumes/234_Johanne_Antoine.pdf\n",
      "HackHer413_Resumes/238_Vinisha_Dhuleshia.pdf\n",
      "HackHer413_Resumes/478_Ishita_Kumar.pdf\n",
      "HackHer413_Resumes/637_Srisuma_Movva.pdf\n",
      "HackHer413_Resumes/311_Matthew_Smith.pdf\n",
      "HackHer413_Resumes/755_Preksha_Mutha.pdf\n",
      "HackHer413_Resumes/396_Mira_Kasari.pdf\n",
      "HackHer413_Resumes/244_Seetha_Chock.pdf\n",
      "HackHer413_Resumes/1016_Akul_Siddalingaswamy.pdf\n",
      "HackHer413_Resumes/421_Colleen_Lam.pdf\n",
      "HackHer413_Resumes/841_rashi_chauhan.pdf\n",
      "HackHer413_Resumes/431_Meghan_Healey.pdf\n",
      "HackHer413_Resumes/178_Yooni_Park.pdf\n",
      "HackHer413_Resumes/428_Amy_Nguyen.pdf\n",
      "HackHer413_Resumes/314_Chen_Jin.pdf\n",
      "HackHer413_Resumes/969_Jules_Kelly.pdf\n",
      "HackHer413_Resumes/756_Cara_Richardson.pdf\n",
      "HackHer413_Resumes/929_Catlinh_Le.pdf\n",
      "HackHer413_Resumes/638_Soumya_Paliwal.pdf\n",
      "HackHer413_Resumes/799_Dhara_Patel.pdf\n",
      "HackHer413_Resumes/429_Denise_Nava.pdf\n",
      "HackHer413_Resumes/657_Isha_Teredsai.pdf\n",
      "HackHer413_Resumes/537_Hailey_Pensky.pdf\n",
      "HackHer413_Resumes/100_Tanisha_Nalavadi.pdf\n",
      "HackHer413_Resumes/703_Julie_Washek.pdf\n",
      "HackHer413_Resumes/555_Tahsina_Saosun.pdf\n",
      "HackHer413_Resumes/507_Kelsie_McAllister.pdf\n",
      "HackHer413_Resumes/568_Jackie_Rowe.pdf\n",
      "HackHer413_Resumes/470_Tejal_Lotlikar.pdf\n",
      "HackHer413_Resumes/778_Akriti_Bhat.pdf\n",
      "HackHer413_Resumes/267_Tina_Liu.pdf\n",
      "HackHer413_Resumes/399_Arianna_Kazemi.pdf\n",
      "HackHer413_Resumes/439_Shyla_Gangwar.pdf\n",
      "HackHer413_Resumes/395_Sharon_Liu.pdf\n",
      "HackHer413_Resumes/699_Ivana_Moore.pdf\n",
      "HackHer413_Resumes/893_Sofia_Marchetti.pdf\n",
      "HackHer413_Resumes/263_Raveena_Mewani.pdf\n",
      "HackHer413_Resumes/900_Anne-Gail_Moreland.pdf\n",
      "HackHer413_Resumes/139_Rupam_Tiwari.pdf\n",
      "HackHer413_Resumes/525_Jaclyn_Acquaviva.pdf\n",
      "HackHer413_Resumes/25_Alicia_Ma_Shum.pdf\n",
      "HackHer413_Resumes/722_Megan_Xie.pdf\n",
      "HackHer413_Resumes/821_Aishwarya_Sudhakar.pdf\n",
      "HackHer413_Resumes/937_Seyedeh_Arta_Razavi.pdf\n",
      "HackHer413_Resumes/788_Chineme_Ezema.pdf\n",
      "HackHer413_Resumes/524_Buyannemekh_Munkhbat.pdf\n",
      "HackHer413_Resumes/403_Allison_Basore.pdf\n",
      "HackHer413_Resumes/208_Julia_Seward.pdf\n",
      "HackHer413_Resumes/800_Sai_Pavitra_Madhurima_Dangati.pdf\n",
      "HackHer413_Resumes/248_Maeve_Newman.pdf\n",
      "HackHer413_Resumes/662_Jenna_Melvin.pdf\n",
      "HackHer413_Resumes/387_Shreya_Ravi.pdf\n",
      "HackHer413_Resumes/186_Corinne_Greene.pdf\n",
      "HackHer413_Resumes/542_SHAN_JIANG.pdf\n",
      "HackHer413_Resumes/660_Angela_Maliakal.pdf\n",
      "HackHer413_Resumes/51_Ruby_Ramsay.pdf\n",
      "HackHer413_Resumes/562_Margaret_Hardin.pdf\n",
      "HackHer413_Resumes/640_Kate_Newcomer.pdf\n",
      "HackHer413_Resumes/869_Diana_Silva.pdf\n",
      "HackHer413_Resumes/306_Koyena_Pal.pdf\n",
      "HackHer413_Resumes/58_Jessica_Fang.pdf\n",
      "HackHer413_Resumes/967_Isabelle_Hodge.pdf\n",
      "HackHer413_Resumes/556_Areeta_Wong.pdf\n",
      "HackHer413_Resumes/882_Sara_Dee.pdf\n",
      "HackHer413_Resumes/621_Christine_Chern.pdf\n",
      "HackHer413_Resumes/540_Ariana_Adames.pdf\n",
      "HackHer413_Resumes/201_Jeana_Choi.pdf\n",
      "HackHer413_Resumes/728_Krista_Cepkauskas.pdf\n",
      "HackHer413_Resumes/352_Linh_Dang.pdf\n",
      "HackHer413_Resumes/926_Nicole_Saleh.pdf\n",
      "HackHer413_Resumes/422_Amee_Trivedi.pdf\n",
      "HackHer413_Resumes/367_Ramya_Sarma.pdf\n",
      "HackHer413_Resumes/737_Isabelle_Nguyen-Phuoc.pdf\n",
      "HackHer413_Resumes/225_Kathleen_Doviken.pdf\n",
      "HackHer413_Resumes/913_Jiaming_Cui.pdf\n",
      "HackHer413_Resumes/486_Tiffany_Tang.pdf\n",
      "HackHer413_Resumes/938_Raveena_Dookhan.pdf\n",
      "HackHer413_Resumes/776_Sushmita_Chaudhari.pdf\n",
      "HackHer413_Resumes/425_Nancy_Agarwal.pdf\n",
      "HackHer413_Resumes/536_Margarita_Gubanova.pdf\n",
      "HackHer413_Resumes/522_Caitlin_Ong.pdf\n",
      "HackHer413_Resumes/119_Lemara_Williams.pdf\n",
      "HackHer413_Resumes/255_Pegah_Rashidnia.pdf\n",
      "HackHer413_Resumes/782_Shanna_Chow.pdf\n",
      "HackHer413_Resumes/476_Caroline_Chen.pdf\n",
      "HackHer413_Resumes/617_Shreya_Pandey.pdf\n",
      "HackHer413_Resumes/630_Sarah_Manlove.pdf\n",
      "HackHer413_Resumes/435_Yixue_Wang.pdf\n",
      "HackHer413_Resumes/789_Jessica_Keast.pdf\n",
      "HackHer413_Resumes/548_Angelina_Shalupova.pdf\n",
      "HackHer413_Resumes/894_Chimene_Minshew.pdf\n",
      "HackHer413_Resumes/886_Memphis_Washington.pdf\n",
      "HackHer413_Resumes/614_Shelby_Allen.pdf\n",
      "HackHer413_Resumes/347_Karina_Popovich.pdf\n",
      "HackHer413_Resumes/472_Carrie_Lamb.pdf\n",
      "HackHer413_Resumes/715_Rachel_Tham.pdf\n",
      "HackHer413_Resumes/710_Claire_Sun.pdf\n",
      "HackHer413_Resumes/394_Michelle_Mantilla.pdf\n",
      "HackHer413_Resumes/392_Soumya_Duriseti.pdf\n",
      "HackHer413_Resumes/86_Unwana_Abasiurua.pdf\n",
      "HackHer413_Resumes/159_Marlena_Keisler.pdf\n",
      "HackHer413_Resumes/787_siyang_tang.pdf\n",
      "HackHer413_Resumes/651_Mariah_Rucker.pdf\n",
      "HackHer413_Resumes/836_Lesley_Zheng.pdf\n",
      "HackHer413_Resumes/155_Sarah_Chou.pdf\n",
      "HackHer413_Resumes/134_Marissa_D-Alonzo.pdf\n",
      "HackHer413_Resumes/264_Ana_Verma.pdf\n",
      "HackHer413_Resumes/257_Anushree_Goswami.pdf\n",
      "HackHer413_Resumes/815_Maggie_Zhang.pdf\n",
      "HackHer413_Resumes/735_Luiza_Menezes.pdf\n",
      "HackHer413_Resumes/753_Vaidehi_Thete.pdf\n",
      "HackHer413_Resumes/503_Julia_Vorobeychik.pdf\n",
      "HackHer413_Resumes/903_Morgan_DiPilla.pdf\n",
      "HackHer413_Resumes/85_Jhinnie_Shome.pdf\n",
      "HackHer413_Resumes/518_Brittany_Laginhas.pdf\n",
      "HackHer413_Resumes/385_Mahima_Beltur.pdf\n",
      "HackHer413_Resumes/121_Grace_Seward.pdf\n",
      "HackHer413_Resumes/752_Audrey_Lai.pdf\n",
      "HackHer413_Resumes/63_Ananya_Ganesh.pdf\n",
      "HackHer413_Resumes/71_Sakshi_Gera.pdf\n",
      "HackHer413_Resumes/224_Amaryce_Osorio.pdf\n",
      "HackHer413_Resumes/49_Jade_Saint-Paul.pdf\n",
      "HackHer413_Resumes/747_Anara_Yegizbayeva.pdf\n",
      "HackHer413_Resumes/307_Rosmery_Izaguirre.pdf\n",
      "HackHer413_Resumes/934_Vincent_Chov.pdf\n",
      "HackHer413_Resumes/375_Kirsten_Lydic.pdf\n",
      "HackHer413_Resumes/888_Sofia_Lis.pdf\n",
      "HackHer413_Resumes/274_Angel_Huang.pdf\n",
      "HackHer413_Resumes/828_Annie_Chi.pdf\n",
      "HackHer413_Resumes/732_Sohini_Kar.pdf\n",
      "HackHer413_Resumes/629_Testing_Testing.pdf\n",
      "HackHer413_Resumes/810_Charlotte_Nickerson.pdf\n",
      "HackHer413_Resumes/95_Samantah_Rydzewski.pdf\n",
      "HackHer413_Resumes/931_Jiaxuan_Ji.pdf\n",
      "HackHer413_Resumes/438_Sajeda_Mokbel.pdf\n",
      "HackHer413_Resumes/104_Manisha_Barnwal.pdf\n",
      "HackHer413_Resumes/852_Neyissa_Exilus.pdf\n",
      "HackHer413_Resumes/745_Betty_Shen.pdf\n",
      "HackHer413_Resumes/455_Rachael_Drinker.pdf\n",
      "HackHer413_Resumes/163_Hannah_Rogers.pdf\n",
      "HackHer413_Resumes/183_Bipasha_Chowdhury.pdf\n",
      "HackHer413_Resumes/214_Fnu_Swati.pdf\n",
      "HackHer413_Resumes/554_Anjali_Patel.pdf\n",
      "HackHer413_Resumes/956_Pooja_Kundaje.pdf\n",
      "HackHer413_Resumes/350_Jordan_Graves.pdf\n",
      "HackHer413_Resumes/221_Shailaja_Sampat.pdf\n",
      "HackHer413_Resumes/962_Pracheta_Boddavaram_Amaranath.pdf\n",
      "HackHer413_Resumes/803_yarlin_lopez.pdf\n",
      "HackHer413_Resumes/293_Roshni_Biswas.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HackHer413_Resumes/259_Sandra_Zhen.pdf\n",
      "HackHer413_Resumes/402_Santhosh_Ramachandran.pdf\n",
      "HackHer413_Resumes/23_Tarang_Mittal.pdf\n",
      "HackHer413_Resumes/19_Kashish_Arora.pdf\n",
      "HackHer413_Resumes/384_Bissenbay_Dauletbayev.pdf\n",
      "HackHer413_Resumes/459_Liza_Bialik.pdf\n",
      "HackHer413_Resumes/479_Luisely_Doza.pdf\n",
      "HackHer413_Resumes/965_Ashley_Artwick.pdf\n",
      "HackHer413_Resumes/211_Linh_Nguyen.pdf\n",
      "HackHer413_Resumes/388_Radhika_Hegde.pdf\n",
      "HackHer413_Resumes/152_Akshaya_Rajagopalan.pdf\n",
      "HackHer413_Resumes/460_Amberly_Lerner.pdf\n",
      "HackHer413_Resumes/634_Nikitha_Preetham.pdf\n",
      "HackHer413_Resumes/996_Alexus_Wang.pdf\n",
      "HackHer413_Resumes/879_Rutuja_Pai.pdf\n",
      "HackHer413_Resumes/219_Anna_Mun.pdf\n",
      "HackHer413_Resumes/526_nikhitha_bonthala.pdf\n",
      "HackHer413_Resumes/484_Hui_Duan.pdf\n",
      "HackHer413_Resumes/156_Lena_Cohen.pdf\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for filename in onlyfiles:\n",
    "  f = filename\n",
    "  filename = mypath+\"/\"+filename\n",
    "  print(filename)\n",
    "  #open allows you to read the file\n",
    "  pdfFileObj = open(filename,'rb')\n",
    "  #The pdfReader variable is a readable object that will be parsed\n",
    "  pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict=False)\n",
    "  #discerning the number of pages will allow us to parse through all #the pages\n",
    "  num_pages = pdfReader.numPages\n",
    "  count = 0\n",
    "  text = \"\"\n",
    "  #The while loop will read each page\n",
    "  while count < num_pages:\n",
    "      pageObj = pdfReader.getPage(count)\n",
    "      count +=1\n",
    "      text += pageObj.extractText()\n",
    "      text = text.replace('\\r','!')\n",
    "      text = text.replace('\\n','')\n",
    "      text = text.replace('\\t','^')\n",
    "      text = text.replace('\\v','*')\n",
    "      text = text.lower()\n",
    "  # split into words by white space\n",
    "  # split into words by white space\n",
    "\n",
    "  # remove punctuation from each word\n",
    "  import re\n",
    "  words = re.split(r'\\W+', text)\n",
    "  #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "  if text != \"\":\n",
    "     text = text\n",
    "  #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "  else:\n",
    "    try:\n",
    "      text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "    except:\n",
    "      text = \"\"\n",
    "  # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "  # Now, we will clean our text variable, and return it as a list of keywords.\n",
    "  texts.append((f, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw dataset: 621\n"
     ]
    }
   ],
   "source": [
    "print('Size of raw dataset:', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "anonymize:\n",
    "\n",
    "DESCRIPTION:\n",
    "takes in tokenized resume and removes identifying information. Approaches task \n",
    "by removing all text before a few 'action' words. This process\n",
    "also conveniently cleans the data of a few garbage tokens.\n",
    "\n",
    "PARAMS:\n",
    "keywords - tokenized data from a single resume\n",
    "\n",
    "RETURN:\n",
    "a copy of keywords with id info scrubbed\n",
    "\n",
    "'''\n",
    "def anonymize(keywords):\n",
    "  lk = len(keywords)\n",
    "  keewords = copy.copy(keywords)\n",
    "  education = ['Education', 'education', 'EDUCATION']\n",
    "  school = ['School', 'school', 'SCHOOL']\n",
    "  experience = ['Experience', 'experience', 'EXPERIENCE']\n",
    "  skills = ['Skills', 'skills', 'SKILLS']\n",
    "  technical = ['Technical', 'technical', 'TECHNICAL']\n",
    "  research = ['Research', 'research', 'RESEARCH']\n",
    "  projects = ['Projects', 'projects', 'PROJECTS']\n",
    "  objective = ['Objective', 'objective', 'OBJECTIVE']\n",
    "  activities = ['Activities', 'activities', 'ACTIVITIES']\n",
    "  interests = ['Interests', 'interests', 'INTERESTS']\n",
    "  for word in range(lk):\n",
    "    if (keywords[word] in education or keywords[word] in experience or\n",
    "        keywords[word] in skills or keywords[word] in technical or\n",
    "       keywords[word] in research or keywords[word] in projects or\n",
    "       keywords[word] in objective or keywords[word] in activities or\n",
    "       keywords[word] in interests):\n",
    "      break\n",
    "    else:\n",
    "      keewords = keewords[1:]\n",
    "  return keewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_false(flag_array, target):\n",
    "  flag_arr = copy.copy(flag_array)\n",
    "  for flag in flag_arr:\n",
    "    if flag is not target:\n",
    "      flag[0] = False\n",
    "  return flag_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "categorize:\n",
    "\n",
    "DESCRIPTION:\n",
    "Sorts anonymized data into general resume categories retaining order\n",
    "\n",
    "PARAMS:\n",
    "keywords - anonymized list of resume data in order-ish\n",
    "\n",
    "RETURN:\n",
    "a dictionary of the categorized resume\n",
    "'''\n",
    "def categorize(keywords):\n",
    "  education = ['Education', 'education', 'EDUCATION', 'School', 'school', 'SCHOOL']\n",
    "  \n",
    "  # Flag to determine both if we run into the word and are in the \n",
    "  # section (flag[0]), as well as if we have seen it before (flag[1])\n",
    "  # Given nature of reumes, first time we encounter these words is \n",
    "  # overwhelmingly the section header\n",
    "  edu = [False, False]\n",
    "  experience = ['Experience', 'experience', 'EXPERIENCE']\n",
    "  exp = [False, False]\n",
    "  skills = ['Skills', 'skills', 'SKILLS', 'Technical', 'technical', 'TECHNICAL']\n",
    "  tech = [False, False]\n",
    "  research = ['Research', 'research', 'RESEARCH']\n",
    "  res = [False, False]\n",
    "  projects = ['Projects', 'projects', 'PROJECTS']\n",
    "  pro = [False, False]\n",
    "  objective = ['Objective', 'objective', 'OBJECTIVE']\n",
    "  obj = [False, False]\n",
    "  activities = ['Activities', 'activities', 'ACTIVITIES']\n",
    "  act = [False, False]\n",
    "  interests = ['Interests', 'interests', 'INTERESTS']\n",
    "  inter = [False, False]\n",
    "  flags = [edu, exp, tech, res, pro, obj, act, inter]\n",
    "  categories_without_skills_and_tech = ['Education', 'education', 'EDUCATION',\n",
    "                                        'School', 'school', 'SCHOOL'\n",
    "                                       'Experience', 'experience', 'EXPERIENCE',\n",
    "                                       'Research', 'research', 'RESEARCH',\n",
    "                                        'Projects', 'projects', 'PROJECTS',\n",
    "                                        'Objective', 'objective', 'OBJECTIVE',\n",
    "                                        'Activities', 'activities', 'ACTIVITIES',\n",
    "                                        'Interests', 'interests', 'INTERESTS']\n",
    "  all_cats = ['Education', 'education', 'EDUCATION',\n",
    "              'School', 'school', 'SCHOOL'\n",
    "              'Experience', 'experience', 'EXPERIENCE',\n",
    "              'Skills', 'skills', 'SKILLS',\n",
    "              'Technical', 'technical', 'TECHNICAL',\n",
    "              'Research', 'research', 'RESEARCH',\n",
    "              'Projects', 'projects', 'PROJECTS',\n",
    "              'Objective', 'objective', 'OBJECTIVE',\n",
    "              'Activities', 'activities', 'ACTIVITIES',\n",
    "              'Interests', 'interests', 'INTERESTS']\n",
    "  \n",
    "  categories = {'education':[], 'experience':[], 'skills':[], 'research':[],\n",
    "                'projects':[], 'objective':[], 'activities':[], 'interests':[]}\n",
    "  words = copy.copy(keywords)\n",
    "  '''\n",
    "  this counter + counter_val are to prevent accidentally going into the next\n",
    "  section\n",
    "  ** in future be sure to check for 'research intern' or 'research assistant' **\n",
    "  '''\n",
    "  counter = 0\n",
    "  count_val = 3\n",
    "  for word in words:\n",
    "    if (word in education and edu[1] == False and counter <= 0):\n",
    "      edu[0] = True\n",
    "      edu[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, edu)\n",
    "    elif (word in experience and exp[1] == False and counter <= 0):\n",
    "      exp[0] = True\n",
    "      exp[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, exp)\n",
    "    elif (word in skills and tech[1] == False and counter <= 0):\n",
    "      tech[0] = True\n",
    "      tech[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, tech)\n",
    "    elif (word in research and res[1] == False and counter <= 0):\n",
    "      res[0] = True\n",
    "      res[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, res)\n",
    "    elif (word in projects and pro[1] == False and counter <= 0):\n",
    "      pro[0] = True\n",
    "      pro[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, pro)\n",
    "    elif (word in objective and obj[1] == False and counter <= 0):\n",
    "      obj[0] = True\n",
    "      obj[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, obj)\n",
    "    elif (word in activities and act[1] == False and counter <= 0):\n",
    "      act[0] = True\n",
    "      act[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, act)\n",
    "    elif (word in interests and inter[1] == False and counter <= 0):\n",
    "      inter[0] = True\n",
    "      inter[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, inter)\n",
    "    \n",
    "    if (edu[0] and word not in education):\n",
    "      categories['education'].append(word)\n",
    "      counter -=1\n",
    "    if (exp[0] and word not in experience):\n",
    "      categories['experience'].append(word)\n",
    "      counter -=1\n",
    "    if (tech[0] and word not in skills):\n",
    "      categories['skills'].append(word)\n",
    "      counter -=1\n",
    "    if (res[0] and word not in research):\n",
    "      categories['research'].append(word)\n",
    "      counter -=1\n",
    "    if (pro[0] and word not in projects):\n",
    "      categories['projects'].append(word)\n",
    "      counter -=1\n",
    "    if (obj[0] and word not in objective):\n",
    "      categories['objective'].append(word)\n",
    "      counter -=1\n",
    "    if (act[0] and word not in activities):\n",
    "      categories['activities'].append(word)\n",
    "      counter -=1\n",
    "    if (inter[0] and word not in interests):\n",
    "      categories['interests'].append(word)\n",
    "      counter -=1\n",
    "  return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of cleaned dataset: 239\n",
      "[('616_Emilyann_Nault.pdf', ['skills', 'certifications', 'technical', 'java', 'python', 'c', '#', 'unity', 'spss', 'adobe', 'photoshop', 'adobe', 'bridge', 'certifications', 'scuba', 'certified', 'may', '2016', 'cpr', 'first', 'aid', 'certified', 'fall', '2014', 'language', 'french', 'intermediate', 'level', 'proficiency', 'education', 'mount', 'holyoke', 'college', 'south', 'hadley', 'gpa', '3.82', 'bachelor', 'arts', 'expected', 'may', '2019', 'intended', 'major', 'computer', 'science', 'neuroscience', '&', 'behavior', 'intended', 'minor', 'psychology', 'honors/awards', 'student', 'liaison', 'neuroscience', 'behavior', 'department', 'mount', 'holyoke', 'college', 'mount', 'holyoke', 'college', 'book', 'award', 'teamsters', 'local', '25', 'scholarship', 'brittany', 'vogler', 'memorial', 'color', 'guard', 'award', 'academic', 'excellence', 'nesba', 'color', 'guard', 'scholarship', 'national', 'honor', 'society', 'hackathons', 'hack', 'holyoke', 'november', '2017', 'relevant', 'experience', 'human-robot', 'interaction', 'independent', 'study', 'interactive', 'computing', 'research', 'lab', 'mount', 'holyoke', 'college', 'fall', '2018', 'supervisor', 'dr.', 'heather', 'pon-barry', '¥', '!', 'conducted', 'literature', 'review', 'simultaneous', 'multi', 'human-robot', 'interaction', '¥', '!', 'designed', 'developed', 'dialogue', 'prototype', 'implemented', 'preexisting', 'system', 'order', 'provide', 'directions', 'around', 'campus', 'human-robot', 'interaction', 'research', 'assistant', 'human-robot', 'interaction', 'lab', 'heriot-watt', 'university', 'summer', '2018', 'supervisor', 'dr.', 'lynne', 'baillie', '¥', '!', 'conducted', 'literature', 'review', 'research', 'topic', '¥', '!', 'designed', 'developed', 'robotic', 'prototype', 'aid', 'upper', 'limb', 'cognitive', 'rehabilitation', 'stroke', 'survivors', '¥', '!', 'directed', 'focus', 'groups', 'interviews', 'physiotherapists', 'occupational', 'therapists', '¥', '!', 'led', 'design', 'workshop', 'test', 'robotic', 'prototype', 'questionnaires', 'usability', 'testing', 'discussion', 'programmer', 'glowlime', 'video', 'game', 'club', 'hampshire', 'college', 'fall', '2017', 'ð', 'spring', '2018', '¥', '!', 'simulated', 'video', 'game', 'company', 'atmosphere', 'created', 'horror', 'based', 'puzzle', 'game', 'two-week', 'period', 'january', 'break', '2018', '¥', '!', 'part', 'team', 'created', '2', 'video', 'games', 'one', 'per', 'semester', 'using', 'unity', 'assistive', 'technology', 'volunteer', 'spaulding', 'rehab', 'hospital', 'charlestown', 'summer', '2017', '¥', '!', 'worked', 'multitude', 'technologies', 'grid3', 'fit', 'needs', 'patientsõ', 'variety', 'diagnoses', '¥', '!', 'assisted', 'occupational', 'therapist', 'patient', 'appointments', 'including', 'treatments', 'clinicõs', 'speech', 'language', 'pathologist', '¥', '!', 'attended', 'hospitalõs', 'monthly', 'business', 'meetings', 'online', 'course', 'certification', 'michigan', 'state', 'university', 'coursera', 'summer', '2017', '¥', '!', 'courses', 'introduction', 'video', 'game', 'development', 'principles', 'game', 'design', 'business', 'games', 'entrepreneurship', 'engineering', 'internship', 'bae', 'systems', 'nashua', 'nh', 'fall', '2014-spring', '2015', '¥', '!', 'worked', 'groups', 'complete', 'various', 'projects', 'presentations', 'concerning', 'different', 'fields', 'engineering', '¥', '!', 'communicated', 'employees', 'variety', 'departments', 'learn', 'business', 'work', 'environment', 'independent', 'studies', 'biotechnology', 'bioethics', 'biotechnology', 'salem', 'high', 'school', 'fall', '2013', 'spring', '2015', '¥', '!', 'created', 'three', 'presentations', 'topics', 'choosing', 'cryonics', 'animal', 'cloning', '¥', '!', 'performed', 'independent', 'labs', 'using', 'techniques', 'gel', 'electrophoresis', 'cloning', 'techniques', 'work', 'experience', 'digitization', 'assistant', 'digital', 'assets', '&', 'preservation', 'services', 'mount', 'holyoke', 'college', 'spring', '2018', '¥', '!', 'capture', 'historical', 'correspondence', 'using', 'digital', 'copy', 'stand', 'edit', 'images', 'photoshop', '¥', '!', 'create', 'metadata', 'digital', 'images', 'online', 'searching', 'presentation', 'dining', 'hall', 'manager', 'mount', 'holyoke', 'college', 'spring', '2017-fall', '2017', '¥', '!', 'act', 'liaison', 'kitchen', 'staff', 'upwards', '60', 'student', 'workers', 'resolve', 'issues', 'may', 'arise', '¥', '!', 'responsible', 'initial', 'training', 'managing', 'opening', 'closing', 'dining', 'hall', '2', '3', 'times', 'week', '¥', '!', 'handle', 'payroll', 'student', 'workers', 'respond', 'emails', 'note-taker', 'mount', 'holyoke', 'college', 'fall', '2016-fall', '2017', 'fall', '2018-present', '¥', '!', 'take', 'notes', 'class', 'students', 'require', 'accessibility', 'services']), ('902_Sanskruti_Shah.pdf', ['education', 'university', 'massachusetts', 'amherst', 'master', 'science', 'computer', 'science', 'gpa', '3.95/4', 'may', '2019', 'relevant', 'courses', 'machine', 'learning', 'computer', 'vision', 'database', 'design', 'implementation', 'intelligent', 'visual', 'computing', 'system', 'test', 'defense', 'sardar', 'patel', 'institute', 'technology', 'bachelor', 'engineering', 'computer', 'engineering', 'cgpa', '8.51/10', 'august', '2017', 'relevant', 'courses', 'artificial', 'intelligence', 'big', 'data', 'analytics', 'soft', 'computing', 'data', 'structures', 'software', 'engineering', 'analysis', 'algorithms', 'distributed', 'databases', 'operating', 'systems', 'languages', 'technologies', 'languages', 'python', 'c', 'java', 'matlab', 'javascript', 'php', 'django', 'reactjs', 'extjs', 'jquery', 'android', 'tools', 'hadoop', 'mapreduce', 'spark', 'tensorflow', 'keras', 'scikit-learn', 'numpy', 'scipy', 'mysql', 'mongodb', 'android', 'studio', 'experience', 'amazon', 'software', 'development', 'intern', 'alexa', 'speech', 'fall', '2018', 'building', 'unified', 'pipeline', 'metadata', 'collection', 'personalization', 'automated', 'speech', 'recognition', 'alexa', 'devices', 'collecting', 'metrics', 'regarding', 'distribution', 'data', 'better', 'model', 'existing', 'input', 'test', 'data', 'goldman', 'sachs', 'technology', 'intern', 'finance', '&', 'risk', 'data', 'engineering', 'summer', '2018', 'built', 'dashboard', 'providing', 'metrics', 'data', 'warehouse', 'allow', 'users', 'access', 'authorized', 'data', 'calculated', 'future', 'projections', 'health', 'system', 'based', 'growth', 'firm', 'relative', 'increase', 'data', 'used', 'reactjs', 'build', 'dashboard', 'java', 'services', 'meta-functional', 'language', 'retrieve', 'data', 'ibm', 'research', 'ai', 'industry', 'independent', 'study', 'spring', '2018', 'made', 'drug-protein', 'binding', 'predictions', 'using', 'deep', 'learning', 'representations', 'chemical', 'structures', 'protein', 'sequences', 'applied', 'graph', 'convolutions', 'drugs', 'temporal', 'convolutions', 'proteins', 'learn', 'chemical', 'fingerprint', 'implemented', 'network', 'predict', 'interaction', 'given', 'learnt', 'protein', 'drug', 'representations', '91', '%', 'accuracy', 'sardar', 'patel', 'institute', 'technology', '-', 'research', 'intern', 'fall', '2016', 'predicted', 'probability', 'psoriasis', 'using', 'machine', 'learning', 'computer', 'department', 'head', 'built', 'system', 'extract', 'features', 'images', 'classify', 'disease', 'presented', 'paper', 'adc', '2016', 'credit', 'suisse', '-', 'technology', 'intern', 'prime', 'services', 'summer', '2016', 'developed', 'multiple', 'modules', 'widgets', 'filtered', 'sorted', 'prioritized', 'data', 'client', 'workstation', 'using', 'extjs', 'implemented', 'ajax', 'calls', 'retrieve', 'json', 'data', 'displayed', 'resizable', 'grid', 'increasing', 'efficiency', 'user', 'friendliness', 'publications', 'artificial', 'neural', 'network', 'approach', 'classification', 'vector-borne', 'diseases', 'machine', 'learning', 'approach', 'classification', 'cardiac', 'arrhythmia', 'authors', 'prajwal', 'shimpi', 'sanskruti', 'shah', 'maitri', 'shroff', 'anand', 'godbole', 'conference', 'ieee', 'international', 'conference', 'computing', 'methodologies', 'communication', 'iccmc', '2017', 'academic', 'projects', 'domain', 'adaptation', 'using', 'adversarial', 'discriminative', 'domain', 'adaptation', 'adda', 'spring', '2018', 'utilized', 'adda', 'combining', 'discriminative', 'modeling', 'untied', 'weight', 'sharing', 'gan', 'loss', 'introduced', 'multi-layer', 'loss', 'exceeding', 'adaptation', 'results', 'standard', 'cross-domain', 'digit', 'classification', 'tasks', '2', '%', '.', 'automated', 'sports', 'highlights', 'using', 'audience', 'reactions', 'fall', '2017', 'generated', 'automatic', 'highlights', 'using', 'hog-svm', 'pre-trained', 'models', '3d', 'convolutional', 'networks', 'using', 'keras', 'obtained', 'accuracy', '26', '%', 'using', 'hog-svm', '67', '%', 'using', 'pre-trained', 'models', '74', '%', 'using', '3d-cnn', 'human', 'disease', 'detection', 'fall', '2016', 'spring', '2017', 'extracted', 'sympt', 'probable', 'vector-borne', 'diseases', 'using', 'svm', 'algorithm', 'detecting', 'irregular', 'arrhythmias', 'ecg', 'signal', 'using', 'ann', 'bpn', 'optimized', 'gradient', 'optimization', 'techniques', 'python', 'best', 'project', 'award', 'represented', 'institute', 'transform', 'maharashtra', 'iet', 'competition', 'conferences', 'grace', 'hopper', 'celebration', '2018', 'attended', 'ghc18', 'september', '26-28', 'held', 'houston', 'tx', 'women', 'engineers', 'code', '2018', 'attended', 'harvard', 'wecode', 'scholarship', 'march', '2-4', 'held', 'cambridge'])]\n"
     ]
    }
   ],
   "source": [
    "tokenized_keywords = []\n",
    "tokenized_categories = []\n",
    "success_files = []\n",
    "c = 0\n",
    "for t in texts:\n",
    "    #The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(t[1])\n",
    "    #we'll create a new list which contains punctuation we wish to clean\n",
    "    punctuations = ['(',')',';',':','[',']',',']\n",
    "    #We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "    stop_words = stopwords.words('english')\n",
    "    #We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "    keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n",
    "    if keywords != []:\n",
    "        k = anonymize(keywords)\n",
    "        if k == []:\n",
    "            c += 1\n",
    "        else:\n",
    "            tokenized_keywords.append((t[0], k))\n",
    "            success_files.append(t[0])\n",
    "random.shuffle(tokenized_keywords)\n",
    "test_data = tokenized_keywords[int((len(x_data)/5)*4):]\n",
    "tokenized_keywords = tokenized_keywords[:int((len(x_data)/5)*4)]\n",
    "print('Size of cleaned dataset:',len(tokenized_keywords))\n",
    "print(tokenized_keywords[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '284_Hoang_Ho.pdf')\n"
     ]
    }
   ],
   "source": [
    "# yx_label contains list of tuples\n",
    "# add new directory name in directory_names (if add new directory in the future)\n",
    "# [(label,filename)]\n",
    "yx_label = []\n",
    "directory_names = [('AI', 0),('fullstack_SWE', 1), ('Hardware', 2), ('Informatics', 3), ('Other', 4), ('inexperienced_SWE', 5),('Web_Developer', 6)]\n",
    "for dir_name in directory_names:\n",
    "  temp = dir_name[0]\n",
    "  filename_list = [f for f in listdir(temp) if isfile(join(temp, f))]\n",
    "  for name in filename_list:\n",
    "    yx_label.append((dir_name[1],name))\n",
    "print(yx_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(file_name, list_of_files):\n",
    "  for f in list_of_files:\n",
    "    # print(f[1])\n",
    "    # print(file_name)\n",
    "    if (f[1] == file_name):\n",
    "      return f\n",
    "  return (4, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1-hot vector for Naive Bayes classifier\n",
    "def vectorize(resume):\n",
    "    vector = np.zeros(14977)\n",
    "    for word in resume:\n",
    "        index = find_id(word)\n",
    "        if (index != -1):\n",
    "            vector[index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build x data\n",
    "labeled_tokenized_keywords = []\n",
    "for resume in tokenized_keywords:\n",
    "    category = find(resume[0], yx_label)\n",
    "    labeled_tokenized_keywords.append((category[0], resume[1]))\n",
    "corpus = []\n",
    "for resume in labeled_tokenized_keywords:\n",
    "    c = resume[1]\n",
    "    corpus.append(c)\n",
    "x_train = []\n",
    "for resume in corpus:\n",
    "    x_train.append(vectorize(resume))\n",
    "x_train = np.array(x_train)\n",
    "    \n",
    "\n",
    "labeled_tokenized_keywords_test = []\n",
    "for resume in test_data:\n",
    "    category = find(resume[0], yx_label)\n",
    "    labeled_tokenized_keywords_test.append((category[0], resume[1]))\n",
    "corpus = []\n",
    "for resume in labeled_tokenized_keywords_test:\n",
    "    c = resume[1]\n",
    "    corpus.append(c)\n",
    "x_test = []\n",
    "for resume in corpus:\n",
    "    x_test.append(vectorize(resume))\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(x_train[0][0:100])\n",
    "print(x_test[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 0 3 3 1 1 3 4 4 6 3 4 6 5 1 3 1 5 4 1 5 0 3 1 4 3 3 1 4 1 3 5 1 5 4\n",
      " 4 1 2 4 3 1 4 4 2 4 3 3 4 4 1 2 4 6 4 3 4 1 0 4 4 5 2 0 3 4 1 4 3 4 6 1 4\n",
      " 4 2 4 1 5 3 1 2 0 5 4 4 4 5 4 3 3 4 6 4 1 5 2 1 4 4 4 4 1 5 4 1 1 4 4 4 4\n",
      " 0 5 6 1 4 2 4 3 1 4 2 4 5 4 1 2 3 3 3 1 1 0 4 4 4 4 3 4 0 4 6 2 6 0 3 3 6\n",
      " 4 4 4 1 4 4 1 4 4 4 5 4 4 1 5 4 5 0 4 4 5 5 0 4 3 4 6 4 4 4 4 4 2 4 5 4 1\n",
      " 3 4 4 5 5 3 1 4 1 2 5 4 3 0 4 1 5 5 5 1 2 4 3 0 2 2 3 4 5 3 4 1 3 4 4 4 4\n",
      " 4 4 3 6 1 1 1 5 4 1 4 1 5 5 2 1 2]\n",
      "[4 4 0 4 5 1 4 3 4 4 4 4 4 5 5 3 5 3 4 4 4 4 4 4 5 0 1 4 0 4 5 4 3 3 4 4 4\n",
      " 4 0 4 4 4 4 4 1 3 4 5 1 4 4 5 6 1 1 4 4 2 4 4]\n"
     ]
    }
   ],
   "source": [
    "# Build y data\n",
    "y_train = []\n",
    "for resume in labeled_tokenized_keywords:\n",
    "    c = resume[0]\n",
    "    y_train.append(c)\n",
    "y_train = np.array(y_train)\n",
    "print(y_train)\n",
    "\n",
    "y_test = []\n",
    "for resume in labeled_tokenized_keywords_test:\n",
    "    c = resume[0]\n",
    "    y_test.append(c)\n",
    "y_test = np.array(y_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 14977)\n",
      "(239,)\n",
      "(60, 14977)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "gnb = nbc()\n",
    "classifier = gnb.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Piece from HW 01 starter code to get a bag-of-words representation\n",
    "Used the next couple code blocks to get the size of vocab, and word counts\n",
    "for each word in each resume.\n",
    "'''\n",
    "def tokenize_resume(resume):\n",
    "    \"\"\"\n",
    "    Tokenize a resume and return its bag-of-words representation.\n",
    "    resume - a string representing a document.\n",
    "    returns a dictionary mapping each word to the number of times it appears in doc.\n",
    "    \"\"\"\n",
    "    bow = defaultdict(float)\n",
    "    tokens = resume\n",
    "    lowered_tokens = map(lambda t: t.lower(), tokens)\n",
    "    for token in lowered_tokens:\n",
    "        bow[token] += 1.0\n",
    "    return dict(bow)\n",
    "\n",
    "def n_word_types(word_counts):\n",
    "    '''\n",
    "    Implement Me!\n",
    "    return a count of all word types in the corpus\n",
    "    using information from word_counts\n",
    "    '''\n",
    "    total = 0\n",
    "    ret_array = []\n",
    "    for word in word_counts:\n",
    "        ret_array.append((word, total))\n",
    "        total += 1\n",
    "    return ret_array\n",
    "\n",
    "\n",
    "def n_word_tokens(word_counts):\n",
    "    '''\n",
    "    Implement Me!\n",
    "    return a count of all word tokens in the corpus\n",
    "    using information from word_counts\n",
    "    '''\n",
    "    total = 0\n",
    "    for word in word_counts:\n",
    "        total += word_counts[word]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resume in \n",
    "word_id_list= n_word_types(tokenized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id(word):\n",
    "    for w in word_id_list:\n",
    "        if (w[0] == word):\n",
    "            return w[1]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all resumes for each category\n",
    "category_labels = [0, 1, 2, 3, 4, 5, 6]\n",
    "cv_values = np.array_split(labeled_tokenized_keywords, 5)\n",
    "categories = {0:defaultdict(float), 1:defaultdict(float), 2:defaultdict(float), 3:defaultdict(float), 4:defaultdict(float), 5:defaultdict(float), 6:defaultdict(float)}\n",
    "category_totals = [0, 0, 0, 0, 0, 0, 0]\n",
    "for category in category_labels:\n",
    "    category_total = 0\n",
    "    for value in range(len(cv_values)-1):\n",
    "#         print(cv_values[value])\n",
    "        for resume in cv_values[value]:\n",
    "#             print(resume)\n",
    "            token_resume = tokenize_resume(resume[2])\n",
    "            if (resume[0] == category):\n",
    "                for word in token_resume:\n",
    "                    categories[category][word] += 1\n",
    "                    category_total += 1\n",
    "    category_totals[category] = category_total\n",
    "    \n",
    "categories_2 = {0:defaultdict(float), 1:defaultdict(float), 2:defaultdict(float), 3:defaultdict(float), 4:defaultdict(float), 5:defaultdict(float), 6:defaultdict(float)}\n",
    "category_totals_2 = [0, 0, 0, 0, 0, 0, 0]\n",
    "for category in category_labels:\n",
    "    category_total = 0\n",
    "    for resume in cv_values[4]:\n",
    "        token_resume = tokenize_resume(resume[2])\n",
    "        if (resume[0] == category):\n",
    "            for word in token_resume:\n",
    "                categories_2[category][word] += 1\n",
    "                category_total += 1\n",
    "    category_totals_2[category] = category_total\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'skills': 7.0, 'undergraduate': 2.0, 'junior': 2.0, '.': 6.0, 'hardworking': 1.0, 'committed': 1.0, 'excited': 1.0, 'computational': 2.0, 'biology': 3.0, '!': 3.0, 'actively': 1.0, 'looking': 1.0, 'genomics': 1.0, 'related': 1.0, 'data': 8.0, 'analytic': 1.0, 'internships': 1.0, 'leadership': 3.0, 'us': 1.0, 'dept': 1.0, 'edu': 1.0, 'psa': 1.0, 'advisor': 1.0, '&': 3.0, 'arts': 3.0, 'liason': 1.0, '|fremont': 1.0, 'gov': 1.0, 'yac': 1.0, '|': 1.0, '7': 1.0, 'years': 1.0, 'dance': 1.0, 'student': 5.0, 'instructor': 1.0, 'technical': 2.0, 'basic': 4.0, 'python': 6.0, 'numpy': 1.0, 'matplotlib': 2.0, 'c++': 2.0, 'java': 5.0, 'r': 4.0, 'matlab': 2.0, '|biochemistry': 1.0, 'education': 8.0, 'b.s': 1.0, 'bioinformatics': 2.0, 'statistics': 3.0, 'minor': 3.0, '@': 1.0, 'uc': 1.0, 'santa': 1.0, 'cruz|': 1.0, 'june': 5.0, '2020': 1.0, 'gpa': 5.0, '3.66': 1.0, 'club': 4.0, 'sisterhood': 1.0, 'stem': 1.0, 'west': 1.0, 'participant': 1.0, 'experience': 7.0, 'autonomics': 1.0, 'solutions': 1.0, 'llc': 1.0, 'software': 3.0, 'engineering': 1.0, 'intern|': 1.0, 'nov': 2.0, '2018': 7.0, 'aws': 2.0, 'tools': 3.0, 'microsoft': 4.0, 'azure': 1.0, 'self': 1.0, 'decode': 1.0, 'research': 5.0, 'writing': 2.0, 'intern': 4.0, 'october': 1.0, '2017': 7.0, 'wrote': 3.0, 'published': 1.0, 'articles': 2.0, 'company': 2.0, 'website': 2.0, 'selfhacked.com': 1.0, 'focus': 1.0, 'biochemical': 1.0, 'pathways': 1.0, 'signal': 1.0, 'transduction': 1.0, 'inflammatory': 1.0, 'response': 1.0, 'oxidative': 1.0, 'damage': 1.0, 'awards': 3.0, 'projects': 5.0, 'web': 3.0, 'development': 3.0, 'designed': 2.0, 'disaster': 1.0, 'relief': 1.0, 'chennai': 1.0, 'india': 3.0, 'flooding': 1.0, 'hackathons': 1.0, 'attended': 2.0, 'broncohacks': 1.0, 'hack': 2.0, 'humanity': 1.0, 'clubs': 2.0, 'society': 3.0, 'women': 2.0, 'maker': 1.0, '3d': 1.0, 'printing': 1.0, 'laser': 1.0, 'cutting': 1.0, 'national': 4.0, 'award': 5.0, 'president': 2.0, 'united': 2.0, 'states': 1.0, 'presidential': 1.0, 'scholar': 2.0, 'titles': 1.0, 'girls': 1.0, 'code': 2.0, 'alumnus': 1.0, 'peer': 2.0, 'health': 3.0, 'educator': 1.0, 'smith': 1.0, 'college': 2.0, 'sep.': 1.0, '2018-present': 1.0, '4.0': 2.0, 'interested': 1.0, 'majoring': 1.0, 'statistical': 2.0, 'sciences': 2.0, 'economics': 1.0, 'took': 1.0, 'introduction': 1.0, 'science': 6.0, 'sds': 1.0, '192': 1.0, 'macroeconomics': 1.0, 'eco': 1.0, '153': 1.0, 'fall': 2.0, '2018.': 1.0, 'currently': 1.0, 'taking': 2.0, 'computer': 6.0, 'programming': 3.0, 'csc': 1.0, '111': 1.0, 'carl': 1.0, 'harrison': 1.0, 'high': 3.0, 'school': 3.0, 'aug.': 1.0, '2014-may': 1.0, 'weighted': 1.0, '4.515': 1.0, 'unweighted': 1.0, '3.939': 1.0, '4.000': 1.0, 'scale': 1.0, 'class': 2.0, 'rank': 1.0, '6': 1.0, '493': 1.0, 'sat': 1.0, 'score': 1.0, '1500': 1.0, '1600': 1.0, 'math': 2.0, '770': 1.0, 'reading': 1.0, '730': 1.0, 'georgia': 1.0, 'institute': 1.0, 'technology': 2.0, 'july': 3.0, '2017-may': 1.0, 'dual': 1.0, 'enrollment': 1.0, 'distance': 1.0, 'program': 5.0, 'linear': 1.0, 'algebra': 1.0, 'dr.': 1.0, 'greg': 1.0, 'mayer': 1.0, 'multivariable': 2.0, 'calculus': 3.0, 'prof.': 1.0, 'doron': 1.0, 'lubinsky': 1.0, 'extracurricular': 2.0, 'activities': 5.0, 'orchestra': 1.0, 'second': 1.0, 'violin': 1.0, 'fourth': 1.0, 'chair': 2.0, 'youth': 2.0, 'symphony': 1.0, 'largest': 1.0, 'southeast': 1.0, 'philharmonia': 1.0, 'highest': 1.0, '2014': 2.0, 'may': 6.0, '2015.': 1.0, '2015': 5.0, 'tri-m': 1.0, 'music': 1.0, 'honor': 2.0, '2015-may': 1.0, 'association': 2.0, 'co-president': 1.0, '12th': 1.0, 'grade': 2.0, 'secretary': 1.0, '11th': 1.0, 'vice': 1.0, 'concertmaster': 1.0, 'chamber': 1.0, 'top': 2.0, 'korean': 1.0, '2006-2016': 1.0, 'western': 1.0, 'massachusetts': 3.0, 'umass': 2.0, 'amherst': 3.0, 'covenant': 1.0, 'one': 2.0, 'year': 3.0, 'moved': 1.0, 'continued': 2.0, 'learned': 1.0, 'language': 2.0, 'culture': 1.0, 'history': 2.0, 'mu': 1.0, 'alpha': 1.0, 'theta': 1.0, '2016-may': 1.0, 'work/volunteer': 1.0, 'federal': 1.0, 'work': 5.0, 'study': 1.0, 'dining': 1.0, 'hall': 1.0, 'worker': 1.0, 'assistant': 5.0, 'teacher': 2.0, 'marietta': 1.0, 'ga': 1.0, 'first': 2.0, 'teachers': 1.0, 'graduate': 1.0, 'assisted': 3.0, 'beginner': 1.0, 'children': 3.0, 'ages': 1.0, '4': 1.0, '6.': 1.0, 'honors': 3.0, 'merit': 3.0, 'commended': 1.0, '50,000': 1.0, 'scorers': 1.0, 'psat/nmsqt': 1.0, '34,000': 1.0, 'selected': 3.0, 'students': 6.0, 'received': 3.0, '1430': 1.0, '2016.': 1.0, 'certificate': 2.0, 'university': 6.0, '5': 2.0, '%': 1.0, 'nominated': 1.0, 'awarded': 2.0, 'naks': 1.0, '1': 2.0, 'schools': 2.0, 'serving': 1.0, '150': 1.0, 'hours': 2.0, 'bs': 1.0, 'commonwealth': 1.0, 'sql': 3.0, 'windows': 2.0, 'os': 1.0, 'mac': 2.0, 'html': 3.0, 'css': 2.0, 'office': 3.0, 'git': 1.0, 'softility': 1.0, 'private': 2.0, 'limited': 1.0, 'network': 1.0, 'engineer': 1.0, 'mar': 1.0, '2016': 5.0, 'jul': 1.0, 'worked': 2.0, 'team': 5.0, '8': 1.0, 'senior': 1.0, 'engineers': 1.0, 'instrumental': 1.0, 'delivering': 1.0, 'analytical': 2.0, 'extraction': 1.0, 'using': 4.0, 'like': 2.0, 'helped': 3.0, 'set': 2.0, 'new': 2.0, 'pcs': 1.0, 'install': 1.0, 'required': 1.0, 'system': 2.0, 'developed': 3.0, 'ongoing': 1.0, 'supervision': 1.0, 'superiors': 1.0, 'inc.': 1.0, 'virginia': 1.0, 'analysis': 3.0, 'jun': 1.0, 'used': 3.0, 'restful': 1.0, 'apis': 1.0, 'elasticsearch': 1.0, 'nosql': 1.0, 'storage': 2.0, 'amazon': 1.0, 'services': 1.0, 'retrieved': 1.0, 'large': 2.0, 'amounts': 1.0, 'configuration': 1.0, 'audit': 1.0, 'rest': 1.0, 'ingested': 1.0, 'indices': 1.0, 'sentini': 1.0, 'hospital': 2.0, 'patient': 1.0, 'coordinator': 1.0, 'jan': 1.0, 'enter': 1.0, 'details': 2.0, 'medical': 1.0, 'records': 2.0, 'via': 1.0, 'information': 2.0, 'called': 1.0, 'suvarna': 1.0, 'outpatient': 1.0, 'systems': 3.0, 'computers': 1.0, 'reception': 1.0, 'patienttracker': 1.0, 'myoutpatient': 1.0, 'number': 1.0, 'date': 1.0, 'discharge': 1.0, 'etc': 2.0, 'customized': 1.0, 'app': 1.0, 'hospitals': 1.0, 'government-funded': 1.0, 'need': 1.0, 'quickly': 1.0, 'allocate': 1.0, 'determine': 1.0, 'insurance': 1.0, 'eligibilities': 1.0, 'accolades': 1.0, 'best': 1.0, 'delegate': 1.0, 'indian': 1.0, 'international': 2.0, 'model': 2.0, 'nations': 1.0, 'representing': 1.0, 'italy': 1.0, 'presenting': 1.0, 'conflicting': 1.0, 'stance': 1.0, 'ban': 1.0, 'lethal': 1.0, 'autonomous': 1.0, 'weapons': 1.0, '1st': 1.0, 'prize': 1.0, 'himakshara': 1.0, 'dr': 1.0, 'a.p.j.abdul': 1.0, 'kalam': 1.0, 'outstanding': 1.0, 'recognized': 1.0, 'terms': 1.0, 'academic': 1.0, 'among': 1.0, 'thousands': 1.0, 'accomplished': 1.0, 'javascript': 2.0, 'duke': 1.0, 'durham': 1.0, 'nc': 2.0, 'bachelor': 4.0, 'marine': 1.0, 'conservation': 1.0, '2021': 1.0, '¥': 2.0, '3.3/4.0': 1.0, 'relevant': 3.0, 'coursework': 2.0, 'structures': 1.0, 'algorithms': 1.0, 'probability': 1.0, 'inference': 1.0, 'machine': 1.0, 'learning': 1.0, 'dhirubhai': 1.0, 'ambani': 1.0, 'mumbai': 1.0, 'diploma': 1.0, 'baccalaureate': 1.0, '41/45': 1.0, 'act-': 1.0, 'composite': 1.0, '33': 1.0, '34': 1.0, '35': 1.0, 'sat-': 1.0, 'mathematics': 2.0, 'ii': 1.0, '780': 1.0, '720': 1.0, 'tech': 1.0, '2018-': 1.0, 'present': 2.0, 'geographic': 1.0, 'early': 1.0, 'career': 1.0, 'developing': 2.0, 'platform': 2.0, 'low': 1.0, 'cost': 1.0, 'animal': 1.0, 'tracking': 1.0, 'specifically': 1.0, 'sea': 1.0, 'turtles': 1.0, 'qr': 1.0, 'codes': 1.0, 'tagging': 1.0, 'unmanned': 1.0, 'aerial': 1.0, 'vehicle': 1.0, 'track': 1.0, 'movement': 2.0, 'executing': 1.0, 'key': 2.0, 'logistical': 1.0, 'marketing': 1.0, 'plans': 1.0, 'annual': 1.0, 'conference': 1.0, '250+': 1.0, 'designing': 2.0, 'procedure': 1.0, 'working': 3.0, 'sponsors': 1.0, 'collaborating': 2.0, 'companies': 1.0, 'reaching': 1.0, 'attendees': 1.0, 'teaching': 2.0, '94.0': 1.0, 'january': 1.0, '52': 1.0, 'introductory': 1.0, 'concepts': 1.0, 'alice3virtual': 1.0, 'world': 2.0, 'debug': 1.0, 'assignments': 2.0, 'held': 1.0, 'volunteer': 2.0, 'reefwatch': 1.0, 'april': 2.0, '2015-': 1.0, 'identified': 1.0, 'indicator': 1.0, 'species': 1.0, 'standard': 1.0, 'operating': 2.0, 'recreational': 1.0, 'divers': 1.0, 'conduct': 1.0, 'underwater': 1.0, 'surveys': 1.0, 'implemented': 2.0, '30': 1.0, 'substrate': 1.0, 'fish': 1.0, 'invertebrate': 1.0, 'setting': 1.0, 'lines': 1.0, 'gauge': 1.0, 'coral': 1.0, 'reefs': 1.0, 'explore': 1.0, 'rehabilitation': 1.0, 'reef': 1.0, 'watch': 1.0, 'art': 1.0, 'local': 2.0, 'directed': 2.0, 'put': 1.0, 'play': 1.0, 'convinced': 1.0, 'government': 1.0, 'implement': 3.0, 'garbage': 1.0, 'collection': 1.0, 'facilities': 1.0, 'address': 1.0, 'issue': 1.0, 'litter': 1.0, 'faced': 1.0, 'community': 1.0, 'dive': 1.0, 'master': 2.0, 'internship': 1.0, 'scuba': 1.0, 'center': 3.0, 'havelock': 1.0, '2016-': 1.0, 'conducted': 3.0, 'diving': 1.0, 'lessons': 1.0, '23': 1.0, 'time': 1.0, 'led': 2.0, '19': 1.0, 'trips': 2.0, 'supervised': 1.0, 'clients': 1.0, 'improved': 1.0, 'workflow': 1.0, 'ensured': 1.0, 'client': 1.0, 'safety': 1.0, 'overseeing': 1.0, 'training': 2.0, 'maintaining': 2.0, 'equipment': 1.0, 'keeping': 1.0, 'facility': 1.0, 'organized': 1.0, 'percolation': 1.0, 'simulated': 1.0, 'water': 1.0, 'drainage': 1.0, 'implementing': 2.0, 'dfs': 1.0, 'union-find': 1.0, 'markov': 1.0, 'uses': 1.0, 'process': 3.0, 'generate': 1.0, 'random': 1.0, 'texts': 1.0, 'based': 1.0, 'provided': 2.0, 'n-body': 1.0, 'simulate': 1.0, 'motion': 1.0, 'n': 1.0, 'objects': 1.0, 'plane': 1.0, 'mutually': 1.0, 'affected': 1.0, 'gravitational': 1.0, 'forces': 1.0, 'animate': 1.0, 'results': 2.0, 'hackathons/': 1.0, 'conferences-': 1.0, 'oceans': 1.0, 'symposium': 2.0, 'blue': 1.0, 'print': 1.0, 'languages': 2.0, 'english': 1.0, 'hindi': 1.0, 'spanish': 3.0, 'marathi': 1.0, 'professional': 2.0, 'certification': 2.0, 'certified': 1.0, 'diver': 1.0, 'wreck': 1.0, 'ppb': 1.0, 'nitrox': 1.0, 'navigation': 1.0, 'completed': 2.0, '120': 1.0, 'dives': 1.0, 'wiring': 1.0, 'duarts': 1.0, 'exec': 1.0, 'board': 2.0, 'delta': 1.0, 'sigma': 1.0, 'pi': 1.0, 'dtech': 1.0, 'associate': 1.0, 'entry': 1.0, 'strong': 1.0, 'administrative': 1.0, 'tasks': 1.0, 'excel': 4.0, 'lead': 3.0, 'well': 1.0, 'follow': 1.0, 'excellent': 1.0, 'communication': 3.0, 'comfortable': 2.0, 'analyst': 1.0, '12/2018': 1.0, '01/2019': 1.0, 'definitive': 1.0, 'healthcare': 1.0, 'framingham': 1.0, 'web-based': 1.0, 'database': 3.0, 'enhanced': 1.0, 'utilizing': 1.0, 'access': 1.0, 'reviewed': 1.0, 'existing': 1.0, 'scripts': 1.0, 'made': 1.0, 'edits': 1.0, 'requested': 1.0, 'specialist': 1.0, '05/2018': 1.0, '08/2018': 1.0, 'collected': 1.0, 'phone': 1.0, 'updated': 1.0, 'pertaining': 1.0, 'physician': 1.0, 'groups': 2.0, 'overall': 1.0, 'cleansing': 1.0, '09/2017': 1.0, '12/2017': 1.0, 'performed': 1.0, 'attendance': 1.0, 'grading': 1.0, 'facilitated': 1.0, 'group': 3.0, 'discussion': 1.0, 'one-on-one': 1.0, 'support': 1.0, 'collaborated': 1.0, 'define': 1.0, 'sessions': 1.0, 'curriculum': 1.0, 'objectives': 1.0, '-': 4.0, 'public': 1.0, 'major': 1.0, '3.88': 1.0, 'additional': 1.0, '3': 2.0, 'member': 1.0, 'active': 1.0, 'minds': 1.0, 'organization': 1.0, 'campus': 1.0, 'weekly': 1.0, 'meetings': 2.0, 'dedicated': 1.0, 'fighting': 1.0, 'stigma': 1.0, 'around': 1.0, 'mental': 1.0, 'illness': 1.0, 'relay': 1.0, 'life': 1.0, 'leader': 3.0, 'raise': 1.0, '$': 1.0, '5000': 1.0, 'american': 1.0, 'cancer': 2.0, 'references': 1.0, 'furnished': 1.0, 'upon': 1.0, 'request': 1.0, 'manipulating': 1.0, 'visualizations': 2.0, 'datasets': 1.0, 'proficient': 1.0, 'command': 1.0, 'line': 1.0, 'environment': 1.0, 'experienced': 1.0, 'applications': 1.0, 'rails': 1.0, 'django': 1.0, 'frameworks': 1.0, 'ohio': 1.0, 'state': 1.0, 'columbus': 1.0, 'oh': 1.0, 'microbiology': 1.0, 'advanced': 1.0, 'management': 1.0, 'application': 1.0, 'design': 1.0, 'intermediate': 1.0, 'i/ii': 1.0, 'intro': 1.0, 'mining': 1.0, 'unix': 1.0, 'git/github': 1.0, 'css/bootstrap': 1.0, 'james': 1.0, 'developer': 1.0, 'sept': 1.0, 'various': 2.0, 'libraries': 1.0, 'pandas': 1.0, 'seaborn': 1.0, 'build': 2.0, 'visualization': 1.0, 'library': 1.0, 'part': 1.0, 'comprehensive': 1.0, 'molecular': 2.0, 'better': 1.0, 'understand': 1.0, 'relationship': 1.0, 'certain': 1.0, 'mutations': 1.0, 'found': 1.0, 'cancer-related': 1.0, 'genes': 1.0, 'next-generation': 1.0, 'sequencing': 2.0, 'supercomputer': 1.0, 'dec': 1.0, 'built': 1.0, 'learn': 1.0, 'topic': 1.0, 'interest': 1.0, 'html/css/javascript': 1.0, 'powerquery': 1.0, 'powerpivot': 1.0, 'consolidate': 1.0, 'numerous': 1.0, 'spreadsheets': 1.0, 'watershed': 1.0, 'documented': 1.0, 'steps': 1.0, 'future': 1.0, 'chemistry': 1.0, 'dept.': 1.0, 'mentor': 1.0, 'oct': 1.0, 'mentored': 1.0, '15': 1.0, '20': 1.0, 'end-of-semester': 1.0, 'project': 2.0, 'answering': 1.0, 'questions': 3.0, 'x-ray': 1.0, 'diffraction': 1.0, 'providing': 1.0, 'guidance': 1.0, 'calculation': 1.0, 'problems': 1.0, 'demonstrating': 1.0, 'safe': 1.0, 'lab': 1.0, 'procedures': 1.0, 'hispanic': 1.0, 'november': 1.0, 'simplified': 1.0, 'nomination': 1.0, 'judging': 1.0, 'eliminating': 1.0, 'incomplete': 1.0, 'submissions': 1.0, 'consolidating': 1.0, 'materials': 2.0, 'good': 2.0, 'hackathon': 1.0, 'hosted': 1.0, 'jpmorgan': 1.0, 'chase': 1.0, 'co.': 1.0, 'camp': 1.0, 'recky': 1.0, 'registration': 1.0, 'spring': 2.0, 'ruby': 1.0, 'streamlining': 1.0, 'osu': 1.0, 'summer': 2.0, 'allowing': 1.0, 'parents': 1.0, 'view': 1.0, 'camps': 1.0, 'enroll': 1.0, 'online': 1.0, 'communicate': 1.0, 'counselors': 1.0, 'necessary': 1.0, 'deployed': 1.0, 'heroku': 1.0, 'maximus': 1.0, 'scholarship': 1.0, '2013': 2.0, 'archery': 1.0, 'coach': 1.0, 'webmaster': 1.0, 'level': 1.0, '2': 2.0, 'usa': 1.0, 'coaching': 1.0, 'returning': 1.0, 'members': 1.0, 'update': 1.0, 'wordpress': 1.0, 'c': 1.0, '#': 1.0, 'perl': 1.0, 'swift': 1.0, 'databases': 1.0, 'mysql': 1.0, 'osx': 1.0, '10': 1.0, 'linux': 1.0, 'coding': 1.0, 'environments': 1.0, 'idle': 1.0, 'eclipse': 1.0, 'xcode': 1.0, 'visual': 1.0, 'studios': 1.0, 'vim': 1.0, 'unity': 1.0, 'focuses': 1.0, 'artificial': 1.0, 'intelligence': 1.0, 'dax': 1.0, 'github': 1.0, 'powerbi': 1.0, 'spotfire': 1.0, 'enabling': 1.0, 'technologies': 2.0, 'bluebird': 1.0, 'bio': 1.0, 'current': 1.0, 'creating': 1.0, '``': 1.0, 'live': 1.0, \"''\": 1.0, 'static': 1.0, 'facilitate': 1.0, 'within': 1.0, 'collaborators': 1.0, 'subject': 1.0, 'matter': 1.0, 'experts': 1.0, 'integrate': 1.0, 'different': 1.0, 'sources': 1.0, 'answer': 1.0, 'scientific': 1.0, 'business': 1.0, 'restructuring': 1.0, 'cleaning': 1.0, 'validating': 1.0, 'cell-based': 1.0, 'manufacturing': 1.0, 'processes': 1.0, 'cellular': 1.0, 'comparability': 1.0, 'studies': 2.0, 'edwin': 1.0, 'california': 1.0, 'irvine': 1.0, 'dec.': 1.0, 'rna': 1.0, 'assembly': 1.0, 'alignment': 1.0, 'transcripts': 1.0, 'reference': 1.0, 'genomes': 1.0, 'improving': 1.0, 'algorithm': 1.0, 'quickmerge': 1.0, 'chakraborty': 1.0, 'et': 1.0, 'al.': 1.0, 'heuristic': 1.0, 'analyzed': 1.0, 'quality': 1.0, 'genome': 1.0, 'assemblies': 1.0, 'merged': 1.0, 'towards': 1.0, 'sky': 1.0, 'repository': 1.0, 'oct.': 1.0, '2d': 1.0, 'fantasy': 1.0, 'role-playing': 1.0, 'game': 1.0, 'focused': 1.0, 'exploring': 1.0, 'morality': 1.0, 'culturally': 1.0, 'diverse': 1.0, 'role': 1.0, 'programmer': 1.0, 'core': 1.0, 'mechanics': 1.0, 'map': 1.0, 'loading': 1.0, 'character': 1.0, 'interactions': 1.0, 'discussions': 1.0, 'implementation': 1.0, 'strategies': 1.0, 'card': 1.0, 'age': 1.0, 'https': 1.0, '//github.com/alongazo/the-card-age/': 1.0, 'sept.': 1.0, 'mix': 1.0, 'tactical': 1.0, 'trading': 1.0, 'inspired': 1.0, 'games': 1.0, 'fire': 1.0, 'emblem': 1.0, 'yu-gi-oh': 1.0, 'course': 1.0, 'a.': 1.0, 'designer': 1.0, 'others': 1.0, 'planning': 1.0, 'playing': 1.0, 'cards': 1.0, 'underlying': 1.0, 'battle': 1.0, 'created': 2.0, 'ui': 1.0, 'enemy': 1.0, 'ai': 1.0, 'battlefield': 1.0, 'lustrous': 1.0, 'legacy': 1.0, '//github.com/afly6899/lustrouslegacy/': 1.0, 'japanese-styled': 1.0, 'final': 1.0, 'prologue': 1.0, 'presented': 2.0, 'uci': 1.0, 'video': 1.0, 'quarter': 1.0, 'established': 1.0, 'events': 1.0, 'maps': 1.0, 'coordinated': 1.0, 'assembled': 1.0, 'product': 1.0, 'northeastern': 1.0, 'expected': 2.0, '2019': 1.0, 'cumulative': 1.0, 'specialization': 1.0, 'biological': 1.0, '3.8': 1.0, 'list': 1.0, 'campuswide': 1.0, 'since': 1.0, 'brown': 1.0, 'universityñprovidence': 1.0, 'ri': 1.0, 'graduation': 1.0, '2022': 1.0, 'candidate': 1.0, 'urban': 1.0, 'chapel': 1.0, 'hill': 1.0, 'schoolñchapel': 1.0, 'valedictorian': 1.0, 'cross': 1.0, 'country': 1.0, 'captain': 1.0, 'elected': 1.0, 'carrboro': 1.0, 'advisory': 1.0, 'courses': 1.0, 'ap': 1.0, 'bc': 1.0, 'lauren': 1.0, 'tappanñcarrboro': 1.0, 'september': 1.0, 'personal': 1.0, 'elderly': 1.0, 'blind': 1.0, 'woman': 1.0, 'read': 1.0, 'managed': 1.0, 'schedule': 1.0, 'internet': 1.0, 'taught': 1.0, 'use': 1.0, 'assistive': 1.0, 'silverspot': 1.0, 'cinemañchapel': 1.0, 'usher': 1.0, 'greeted': 1.0, 'guests': 1.0, 'proper': 1.0, 'theater': 1.0, 'handled': 1.0, 'customer': 1.0, 'complaints': 1.0, 'special': 1.0, 'needs': 1.0, 'unc': 1.0, 'parr': 1.0, 'ethicsñchapel': 1.0, 'past': 1.0, 'ethics': 1.0, 'bowl': 1.0, 'cases': 1.0, 'summarized': 1.0, '150+': 1.0, 'written': 1.0, '2012': 1.0, 'moral': 1.0, 'philosophy': 1.0, 'educational': 1.0, 'resources': 1.0, 'teams': 1.0, 'promotional': 1.0, 'pharmacology': 1.0, 'labñchapel': 1.0, '17': 1.0, 'participants': 1.0, 'eshelman': 1.0, 'pharmacyõs': 1.0, 'young': 1.0, 'innovators': 1.0, 'yip': 1.0, 'investigated': 1.0, 'effect': 1.0, 'pluronics': 1.0, 'gene': 1.0, 'transfer': 1.0, 'drug': 1.0, 'delivery': 1.0, 'paper': 1.0, 'ignite': 1.0, 'cs': 1.0, 'teach': 1.0, 'underrepresented': 1.0, 'backgrounds': 1.0, 'elementary': 1.0, 'daily': 1.0, 'herald': 1.0, 'graphics': 1.0, 'creator': 1.0, 'visually': 1.0, 'represent': 1.0, 'referenced': 1.0, 'news': 1.0, 'outing': 1.0, 'trip': 1.0, 'plan': 1.0, 'off-campus': 1.0, 'hiking': 1.0, 'backpacking': 1.0, 'trail': 1.0, 'running': 1.0, 'accessible': 1.0, 'long-distance': 1.0, 'marathons': 1.0, 'half': 1.0, 'adobe': 1.0, 'illustrator': 1.0, 'full': 1.0, 'proficiency': 1.0}) 1514\n"
     ]
    }
   ],
   "source": [
    "print(categories_2[3], category_totals_2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
