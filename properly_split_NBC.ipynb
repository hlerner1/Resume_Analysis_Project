{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hlerner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hlerner/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import copy\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.naive_bayes import GaussianNB as nbc\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from collections import *\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<names of resume files>\n"
     ]
    }
   ],
   "source": [
    "# This line refreshes the known data in the resume folder\n",
    "!ls \"HackHer413_Resumes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mypath = \"HackHer413_Resumes\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<names of resume files>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<names of resume files>\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for filename in onlyfiles:\n",
    "  f = filename\n",
    "  filename = mypath+\"/\"+filename\n",
    "  print(filename)\n",
    "  #open allows you to read the file\n",
    "  pdfFileObj = open(filename,'rb')\n",
    "  #The pdfReader variable is a readable object that will be parsed\n",
    "  pdfReader = PyPDF2.PdfFileReader(pdfFileObj, strict=False)\n",
    "  #discerning the number of pages will allow us to parse through all #the pages\n",
    "  num_pages = pdfReader.numPages\n",
    "  count = 0\n",
    "  text = \"\"\n",
    "  #The while loop will read each page\n",
    "  while count < num_pages:\n",
    "      pageObj = pdfReader.getPage(count)\n",
    "      count +=1\n",
    "      text += pageObj.extractText()\n",
    "      text = text.replace('\\r','!')\n",
    "      text = text.replace('\\n','')\n",
    "      text = text.replace('\\t','^')\n",
    "      text = text.replace('\\v','*')\n",
    "      text = text.lower()\n",
    "  # split into words by white space\n",
    "  # split into words by white space\n",
    "\n",
    "  # remove punctuation from each word\n",
    "  import re\n",
    "  words = re.split(r'\\W+', text)\n",
    "  #This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "  if text != \"\":\n",
    "     text = text\n",
    "  #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "  else:\n",
    "    try:\n",
    "      text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "    except:\n",
    "      text = \"\"\n",
    "  # Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "  # Now, we will clean our text variable, and return it as a list of keywords.\n",
    "  texts.append((f, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of raw dataset: 621\n"
     ]
    }
   ],
   "source": [
    "print('Size of raw dataset:', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "anonymize:\n",
    "\n",
    "DESCRIPTION:\n",
    "takes in tokenized resume and removes identifying information. Approaches task \n",
    "by removing all text before a few 'action' words. This process\n",
    "also conveniently cleans the data of a few garbage tokens.\n",
    "\n",
    "PARAMS:\n",
    "keywords - tokenized data from a single resume\n",
    "\n",
    "RETURN:\n",
    "a copy of keywords with id info scrubbed\n",
    "\n",
    "'''\n",
    "def anonymize(keywords):\n",
    "  lk = len(keywords)\n",
    "  keewords = copy.copy(keywords)\n",
    "  education = ['Education', 'education', 'EDUCATION']\n",
    "  school = ['School', 'school', 'SCHOOL']\n",
    "  experience = ['Experience', 'experience', 'EXPERIENCE']\n",
    "  skills = ['Skills', 'skills', 'SKILLS']\n",
    "  technical = ['Technical', 'technical', 'TECHNICAL']\n",
    "  research = ['Research', 'research', 'RESEARCH']\n",
    "  projects = ['Projects', 'projects', 'PROJECTS']\n",
    "  objective = ['Objective', 'objective', 'OBJECTIVE']\n",
    "  activities = ['Activities', 'activities', 'ACTIVITIES']\n",
    "  interests = ['Interests', 'interests', 'INTERESTS']\n",
    "  for word in range(lk):\n",
    "    if (keywords[word] in education or keywords[word] in experience or\n",
    "        keywords[word] in skills or keywords[word] in technical or\n",
    "       keywords[word] in research or keywords[word] in projects or\n",
    "       keywords[word] in objective or keywords[word] in activities or\n",
    "       keywords[word] in interests):\n",
    "      break\n",
    "    else:\n",
    "      keewords = keewords[1:]\n",
    "  return keewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_false(flag_array, target):\n",
    "  flag_arr = copy.copy(flag_array)\n",
    "  for flag in flag_arr:\n",
    "    if flag is not target:\n",
    "      flag[0] = False\n",
    "  return flag_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "categorize:\n",
    "\n",
    "DESCRIPTION:\n",
    "Sorts anonymized data into general resume categories retaining order\n",
    "\n",
    "PARAMS:\n",
    "keywords - anonymized list of resume data in order-ish\n",
    "\n",
    "RETURN:\n",
    "a dictionary of the categorized resume\n",
    "'''\n",
    "def categorize(keywords):\n",
    "  education = ['Education', 'education', 'EDUCATION', 'School', 'school', 'SCHOOL']\n",
    "  \n",
    "  # Flag to determine both if we run into the word and are in the \n",
    "  # section (flag[0]), as well as if we have seen it before (flag[1])\n",
    "  # Given nature of reumes, first time we encounter these words is \n",
    "  # overwhelmingly the section header\n",
    "  edu = [False, False]\n",
    "  experience = ['Experience', 'experience', 'EXPERIENCE']\n",
    "  exp = [False, False]\n",
    "  skills = ['Skills', 'skills', 'SKILLS', 'Technical', 'technical', 'TECHNICAL']\n",
    "  tech = [False, False]\n",
    "  research = ['Research', 'research', 'RESEARCH']\n",
    "  res = [False, False]\n",
    "  projects = ['Projects', 'projects', 'PROJECTS']\n",
    "  pro = [False, False]\n",
    "  objective = ['Objective', 'objective', 'OBJECTIVE']\n",
    "  obj = [False, False]\n",
    "  activities = ['Activities', 'activities', 'ACTIVITIES']\n",
    "  act = [False, False]\n",
    "  interests = ['Interests', 'interests', 'INTERESTS']\n",
    "  inter = [False, False]\n",
    "  flags = [edu, exp, tech, res, pro, obj, act, inter]\n",
    "  categories_without_skills_and_tech = ['Education', 'education', 'EDUCATION',\n",
    "                                        'School', 'school', 'SCHOOL'\n",
    "                                       'Experience', 'experience', 'EXPERIENCE',\n",
    "                                       'Research', 'research', 'RESEARCH',\n",
    "                                        'Projects', 'projects', 'PROJECTS',\n",
    "                                        'Objective', 'objective', 'OBJECTIVE',\n",
    "                                        'Activities', 'activities', 'ACTIVITIES',\n",
    "                                        'Interests', 'interests', 'INTERESTS']\n",
    "  all_cats = ['Education', 'education', 'EDUCATION',\n",
    "              'School', 'school', 'SCHOOL'\n",
    "              'Experience', 'experience', 'EXPERIENCE',\n",
    "              'Skills', 'skills', 'SKILLS',\n",
    "              'Technical', 'technical', 'TECHNICAL',\n",
    "              'Research', 'research', 'RESEARCH',\n",
    "              'Projects', 'projects', 'PROJECTS',\n",
    "              'Objective', 'objective', 'OBJECTIVE',\n",
    "              'Activities', 'activities', 'ACTIVITIES',\n",
    "              'Interests', 'interests', 'INTERESTS']\n",
    "  \n",
    "  categories = {'education':[], 'experience':[], 'skills':[], 'research':[],\n",
    "                'projects':[], 'objective':[], 'activities':[], 'interests':[]}\n",
    "  words = copy.copy(keywords)\n",
    "  '''\n",
    "  this counter + counter_val are to prevent accidentally going into the next\n",
    "  section\n",
    "  ** in future be sure to check for 'research intern' or 'research assistant' **\n",
    "  '''\n",
    "  counter = 0\n",
    "  count_val = 3\n",
    "  for word in words:\n",
    "    if (word in education and edu[1] == False and counter <= 0):\n",
    "      edu[0] = True\n",
    "      edu[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, edu)\n",
    "    elif (word in experience and exp[1] == False and counter <= 0):\n",
    "      exp[0] = True\n",
    "      exp[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, exp)\n",
    "    elif (word in skills and tech[1] == False and counter <= 0):\n",
    "      tech[0] = True\n",
    "      tech[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, tech)\n",
    "    elif (word in research and res[1] == False and counter <= 0):\n",
    "      res[0] = True\n",
    "      res[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, res)\n",
    "    elif (word in projects and pro[1] == False and counter <= 0):\n",
    "      pro[0] = True\n",
    "      pro[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, pro)\n",
    "    elif (word in objective and obj[1] == False and counter <= 0):\n",
    "      obj[0] = True\n",
    "      obj[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, obj)\n",
    "    elif (word in activities and act[1] == False and counter <= 0):\n",
    "      act[0] = True\n",
    "      act[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, act)\n",
    "    elif (word in interests and inter[1] == False and counter <= 0):\n",
    "      inter[0] = True\n",
    "      inter[1] = True\n",
    "      counter = count_val\n",
    "      flags = make_false(flags, inter)\n",
    "    \n",
    "    if (edu[0] and word not in education):\n",
    "      categories['education'].append(word)\n",
    "      counter -=1\n",
    "    if (exp[0] and word not in experience):\n",
    "      categories['experience'].append(word)\n",
    "      counter -=1\n",
    "    if (tech[0] and word not in skills):\n",
    "      categories['skills'].append(word)\n",
    "      counter -=1\n",
    "    if (res[0] and word not in research):\n",
    "      categories['research'].append(word)\n",
    "      counter -=1\n",
    "    if (pro[0] and word not in projects):\n",
    "      categories['projects'].append(word)\n",
    "      counter -=1\n",
    "    if (obj[0] and word not in objective):\n",
    "      categories['objective'].append(word)\n",
    "      counter -=1\n",
    "    if (act[0] and word not in activities):\n",
    "      categories['activities'].append(word)\n",
    "      counter -=1\n",
    "    if (inter[0] and word not in interests):\n",
    "      categories['interests'].append(word)\n",
    "      counter -=1\n",
    "  return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of cleaned dataset: 239\n",
      "[('<name of resume file>\n', ['education', 'university', 'massachusetts', 'amherst', 'college', 'humanities', 'fine', 'arts', 'fall', '2018', '-', 'present', 'major', 'computational', 'linguistics', 'relevant', 'courses', 'introduction', 'computational', 'linguistics', 'introduction', 'syntax', 'reasoning', 'uncertainty', 'spring', '2019', 'introduction', 'semantics', 'spring', '2019', 'syntax', 'dialects', 'english', 'spring', '2019', 'cornell', 'university', 'college', 'engineering', 'ithaca', 'ny', 'fall', '2015', 'spring', '2017', 'major', 'computer', 'science', 'linguistics', 'relevant', 'courses', 'introduction', 'computing', 'using', 'matlab', 'object-oriented', 'programming', 'data', 'structures', 'discrete', 'structures', 'multivariable', 'calculus', 'differential', 'equations', 'linear', 'algebra', 'introduction', 'linguistics', 'introduction', 'phonetics', 'phonology', 'boston', 'university', 'boston', 'summer', '2017', '6-week', 'historical', 'linguistics', 'american', 'university', 'armenia', 'yerevan', 'armenia', 'summer', '2016', '5-week', 'summer', 'program', 'armenian', 'language', 'culture', 'global', 'perspectives', 'site', 'stories', 'skills', '_', 'software', 'familiarity', 'matlab', 'java', 'python', 'c', 'proficient', 'microsoft', 'office', 'suite', 'languages', 'proficient', 'french', 'attic', 'greek', 'exposure', 'armenian', 'mandarin', 'work', 'experience', 'school', 'teacher', 'elizabeth', 'ann', 'clune', 'montessori', 'school', 'ithaca', 'ithaca', 'ny', 'fall', '2017', 'spring', '2018', 'supervised', 'preschool', 'elementary', 'age', 'children', 'led', 'activities', 'instructed', 'guided', 'students', 'worked', 'closely', 'instructors', 'worked', 'front', 'desk', 'intern', 'laughing', 'goat', 'fiber', 'farm', 'ithaca', 'ny', 'oct.', '2017', 'dec.', '2017', 'assisted', 'aspects', 'small', 'farming', 'fiber', 'work', 'animal', 'husbandry', 'care', 'worked', 'cooperatively', 'employees', 'interns', 'creatively', 'solve', 'problems', 'lab', 'assistant', 'cornell', 'university', 'netravali', 'labs', 'spring', '2016', 'fall', '2016', 'developed', 'experimental', 'sustainable', 'procedures', 'treat', 'cotton', 'wool', 'fabrics', 'resist', 'wrinkling', 'used', 'tensile', 'testing', 'machine', 'autoclave', 'crease', 'recovery', 'tester', 'etc.', 'analyzed', 'sample', 'data', 'activities', 'stem', 'umass', 'amherst', 'fall', '2018', '-', 'present', 'participates', 'organized', 'events', 'workshops', 'panels', 'related', 'career', 'advancement', 'lgbtq+', 'stem', 'students', 'attending', 'ostem', '2018', 'national', 'conference', 'houston', 'tx', 'flute', '2007', '-', 'present', 'performs', 'year-round', 'ensembles', 'schools', 'festivals', 'local', 'performing', 'arts', 'organizations']), ('632_Thanathorn_Sukprasert.pdf', ['skills', 'well', 'interest', 'computer', 'hardware', 'software', '.', 'education', 'university', 'massachusetts', 'amherst', '!', '!', '!', '!', 'anticipated', 'may', '2022', 'bachelor', 'science', 'computer', 'engineering', 'gpa:3.79', 'chancellor', 'award', 'relevant', 'courses', 'introduction', 'electrical', '&', 'computer', 'systems', 'engineering', 'physics', 'calculus', 'engineering', 'freshman', 'seminar', 'academic', 'projects', 'introduction', 'electrical', '&', 'computer', 'systems', 'engineering', '¥worked', 'pair', 'create', 'countdown', 'clock', 'counts', 'time', 'seconds', '.', 'user', 'gets', 'input', 'time', 'oneõs', 'choice', 'choosing', 'four', 'digits', 'numbers', '.', 'counting', 'done', 'led', 'lights', '.', '¥the', 'successful', 'countdown', 'clock', 'demonstrated', 'faculty', 'peers', 'experience', '¥student', 'leadership', 'executive', 'board', 'president', '2017', '-', '2018', '¥served', 'outdoor', 'leader', '2014', '-', '2017', '¥committee', 'student-initiated', 'project', 'approved', 'schoolñcleaners', 'appreciation', '2017', 'skills', 'microsoft', 'excel', 'microsoft', 'word', 'microsoft', 'powerpoint', 'matlab', 'volunteer', 'experience', 'cultural', 'immersion', 'vietnam', '2017', '¥visited', 'orphanage', 'help', 'taking', 'care', 'agent-orange', 'victims', 'donated', 'basic', 'necessities'])]\n"
     ]
    }
   ],
   "source": [
    "tokenized_keywords = []\n",
    "tokenized_categories = []\n",
    "success_files = []\n",
    "c = 0\n",
    "for t in texts:\n",
    "    #The word_tokenize() function will break our text phrases into #individual words\n",
    "    tokens = word_tokenize(t[1])\n",
    "    #we'll create a new list which contains punctuation we wish to clean\n",
    "    punctuations = ['(',')',';',':','[',']',',']\n",
    "    #We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "    stop_words = stopwords.words('english')\n",
    "    #We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "    keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n",
    "    if keywords != []:\n",
    "        k = anonymize(keywords)\n",
    "        if k == []:\n",
    "            c += 1\n",
    "        else:\n",
    "            tokenized_keywords.append((t[0], k))\n",
    "            success_files.append(t[0])\n",
    "random.shuffle(tokenized_keywords)\n",
    "test_data = tokenized_keywords[int((len(tokenized_keywords)/5)*4):]\n",
    "tokenized_keywords = tokenized_keywords[:int((len(tokenized_keywords)/5)*4)]\n",
    "print('Size of cleaned dataset:',len(tokenized_keywords))\n",
    "print(tokenized_keywords[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '284_Hoang_Ho.pdf')\n"
     ]
    }
   ],
   "source": [
    "# yx_label contains list of tuples\n",
    "# add new directory name in directory_names (if add new directory in the future)\n",
    "# [(label,filename)]\n",
    "yx_label = []\n",
    "directory_names = [('AI', 0),('fullstack_SWE', 1), ('Hardware', 2), ('Informatics', 3), ('Other', 4), ('inexperienced_SWE', 5),('Web_Developer', 6)]\n",
    "for dir_name in directory_names:\n",
    "  temp = dir_name[0]\n",
    "  filename_list = [f for f in listdir(temp) if isfile(join(temp, f))]\n",
    "  for name in filename_list:\n",
    "    yx_label.append((dir_name[1],name))\n",
    "print(yx_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(file_name, list_of_files):\n",
    "  for f in list_of_files:\n",
    "    if (f[1] == file_name):\n",
    "      return f\n",
    "  return (4, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12633\n"
     ]
    }
   ],
   "source": [
    "big_list = []\n",
    "for resume in tokenized_keywords:\n",
    "    for word in resume[1]:\n",
    "        big_list.append(word)\n",
    "big_list = set(big_list)\n",
    "c = 0\n",
    "print(len(big_list))\n",
    "vocab_size = len(big_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ids(big_list_of_words):\n",
    "    ids = []\n",
    "    count = 0\n",
    "    for word in big_list_of_words:\n",
    "        val = (count, word)\n",
    "        ids.append(val)\n",
    "        count += 1\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = assign_ids(big_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id(word):\n",
    "    for w in word_ids:\n",
    "        if (w[1] == word):\n",
    "            return w[0]\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build one-hot vector with appropriate vocab size for padding\n",
    "def vectorize(resume):\n",
    "    vector = np.zeros(vocab_size)\n",
    "    for word in resume:\n",
    "        index = find_id(word)\n",
    "        if (index != -1):\n",
    "            vector[index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build x data data\n",
    "labeled_tokenized_keywords = []\n",
    "for resume in tokenized_keywords:\n",
    "    category = find(resume[0], yx_label)\n",
    "    labeled_tokenized_keywords.append((category[0], resume[1]))\n",
    "corpus = []\n",
    "for resume in labeled_tokenized_keywords:\n",
    "    c = resume[1]\n",
    "    corpus.append(c)\n",
    "x_train = []\n",
    "for resume in corpus:\n",
    "    x_train.append(vectorize(resume))\n",
    "x_train = np.array(x_train)\n",
    "    \n",
    "\n",
    "labeled_tokenized_keywords_test = []\n",
    "for resume in test_data:\n",
    "    category = find(resume[0], yx_label)\n",
    "    labeled_tokenized_keywords_test.append((category[0], resume[1]))\n",
    "corpus = []\n",
    "for resume in labeled_tokenized_keywords_test:\n",
    "    c = resume[1]\n",
    "    corpus.append(c)\n",
    "x_test = []\n",
    "for resume in corpus:\n",
    "    x_test.append(vectorize(resume))\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(x_train[0][0:500])\n",
    "print(x_test[0][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 4 2 5 5 4 5 4 4 1 3 4 3 0 4 1 4 3 1 2 4 2 1 4 5 2 3 4 4 6 5 3 3 5 5\n",
      " 6 5 1 5 4 3 4 5 5 2 3 4 1 4 4 5 4 4 3 4 3 5 1 5 1 4 3 4 4 4 4 5 2 4 4 0 3\n",
      " 0 6 4 4 4 6 4 3 6 1 4 1 5 2 4 6 1 1 4 3 6 0 4 4 1 1 4 0 4 4 3 5 4 1 3 5 4\n",
      " 0 4 5 2 3 3 4 2 4 4 4 1 4 4 4 3 4 3 4 5 5 2 3 4 5 4 4 4 1 4 5 2 4 1 4 4 4\n",
      " 0 4 2 4 4 1 6 5 4 4 5 4 3 4 1 4 2 4 1 6 0 2 6 4 4 4 4 1 6 0 6 0 0 5 3 4 4\n",
      " 4 2 1 4 1 4 4 3 3 1 4 1 3 4 2 1 3 3 5 4 4 1 4 4 4 4 1 5 1 3 0 1 3 3 4 1 1\n",
      " 4 3 4 2 1 4 4 4 4 4 4 4 5 4 1 4 1]\n",
      "[4 4 3 4 4 4 4 1 1 4 3 4 1 3 4 1 3 4 3 5 4 0 2 1 5 1 3 4 0 1 0 4 4 4 4 4 1\n",
      " 4 4 3 4 1 4 1 5 5 0 4 3 1 5 4 5 1 5 4 5 5 0 4]\n"
     ]
    }
   ],
   "source": [
    "# build y data\n",
    "y_train = []\n",
    "for resume in labeled_tokenized_keywords:\n",
    "    c = resume[0]\n",
    "    y_train.append(c)\n",
    "y_train = np.array(y_train)\n",
    "print(y_train)\n",
    "\n",
    "y_test = []\n",
    "for resume in labeled_tokenized_keywords_test:\n",
    "    c = resume[0]\n",
    "    y_test.append(c)\n",
    "y_test = np.array(y_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 12633)\n",
      "(239,)\n",
      "(60, 12633)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "# Classifier\n",
    "gnb = nbc()\n",
    "classifier = gnb.fit(x_train, y_train)\n",
    "score = classifier.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
